"""
ç”¨æˆ·ç”»åƒåˆ†æå™¨

ä½¿ç”¨ LangGraph å®ç°ç”¨æˆ·ç”»åƒåˆ†æçš„ä¸šåŠ¡é€»è¾‘ã€‚
"""

import time
from typing import AsyncGenerator, List, Optional, Dict, Any, Tuple
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage
from pydantic import SecretStr

from util import (
    config, logger, ProfileAnalysisRequest, AgentResponse,
    ThinkingResponse, AnswerResponse, ProfileResponse, ErrorResponse, UserProfileCompleteResponse,
    ChatMessage, UserProfile, FeatureData
)
from util.database import db_manager
from util.memory_manager import memory_manager


# ç”¨æˆ·ç‰¹å¾å­—å…¸ç±»å‹ï¼šfeature_name -> FeatureData
UserFeatures = Dict[str, FeatureData]


class ExistingFeatureForLLM(TypedDict):
    """ä¼ é€’ç»™ LLM çš„å·²æœ‰ç‰¹å¾æ•°æ®ç±»å‹"""
    category_name: str
    feature_name: str
    feature_value: Any
    confidence: float


class LLMExtractedFeature(TypedDict):
    """LLM æå–çš„ç‰¹å¾ç±»å‹"""
    value: Any
    confidence: float
    reasoning: str


class ProfileAnalyzerState(TypedDict):
    """ç”¨æˆ·ç”»åƒåˆ†æå™¨çŠ¶æ€ç±»"""
    user_id: int
    session_id: int
    history_chats: List[ChatMessage]
    custom_profile: Optional[UserProfile]
    existing_features: UserFeatures
    extracted_features: UserFeatures
    completion_status: Dict[str, Any]
    suggested_questions: Optional[str]
    final_answer: Optional[str]
    is_complete: bool
    asked_questions_history: List[str]  # è®°å½•ä¹‹å‰é—®è¿‡çš„é—®é¢˜
    asked_features_history: List[str]   # è®°å½•ä¹‹å‰è¯¢é—®è¿‡çš„ç‰¹å¾åç§°


class ProfileAnalyzer:
    """ç”¨æˆ·ç”»åƒåˆ†æå™¨ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        # ç”¨æˆ·å†å²é—®é¢˜è®°å½• - æŒ‰ç”¨æˆ·IDå­˜å‚¨
        self.user_questions_history: Dict[int, List[str]] = {}
        self.user_features_history: Dict[int, List[str]] = {}

        self.llm = ChatOpenAI(
            api_key=SecretStr(
                config.OPENAI_API_KEY) if config.OPENAI_API_KEY else None,
            base_url=config.OPENAI_BASE_URL,
            model=config.OPENAI_MODEL,
            temperature=0.7
        )
        self.workflow = self._build_workflow()

        # ç‰¹å¾åˆ†ç±»å®šä¹‰
        self.feature_categories = {
            'basic_identity': [
                'gender', 'date_of_birth', 'marital_status',
                'residence_city', 'occupation_type', 'industry'
            ],
            'female_specific': [
                'pregnancy_status', 'childbearing_plan'
            ],
            'family_structure': [
                'family_structure', 'number_of_children', 'caregiving_responsibility',
                'monthly_household_expense', 'mortgage_balance', 'is_family_financial_support'
            ],
            'financial_status': [
                'annual_total_income', 'income_stability', 'annual_insurance_budget'
            ],
            'health_lifestyle': [
                'overall_health_status', 'has_chronic_disease', 'smoking_status', 'recent_medical_checkup'
            ]
        }

        # å¿…è¦ç‰¹å¾å®šä¹‰
        self.required_features = {
            'gender', 'date_of_birth', 'marital_status', 'occupation_type', 'industry',
            'family_structure', 'monthly_household_expense', 'is_family_financial_support',
            'annual_total_income', 'income_stability', 'annual_insurance_budget', 'overall_health_status'
        }

        # å­—æ®µå€¼æ ‡å‡†åŒ–è§„åˆ™
        self.field_normalization_rules = {
            'gender': {
                'valid_values': ['ç”·', 'å¥³'],
                'mappings': {
                    'ç”·æ€§': 'ç”·', 'male': 'ç”·', 'M': 'ç”·', 'm': 'ç”·',
                    'å¥³æ€§': 'å¥³', 'female': 'å¥³', 'F': 'å¥³', 'f': 'å¥³'
                }
            },
            'marital_status': {
                'valid_values': ['æœªå©š', 'å·²å©š', 'ç¦»å¼‚', 'ä¸§å¶'],
                'mappings': {
                    'å•èº«': 'æœªå©š', 'æœªç»“å©š': 'æœªå©š', 'å•èº«ç‹—': 'æœªå©š',
                    'ç»“å©š': 'å·²å©š', 'ç»“å©šäº†': 'å·²å©š', 'æœ‰é…å¶': 'å·²å©š',
                    'ç¦»å©š': 'ç¦»å¼‚', 'ç¦»å©šäº†': 'ç¦»å¼‚', 'åˆ†å±…': 'ç¦»å¼‚',
                    'ä¸§å¶äº†': 'ä¸§å¶', 'å¤±å¶': 'ä¸§å¶'
                }
            },
            'occupation_type': {
                'valid_values': ['ä¼ä¸šå‘˜å·¥', 'å…¬åŠ¡å‘˜', 'ä¸ªä½“ç»è¥', 'ä¼ä¸šä¸»', 'è‡ªç”±èŒä¸š', 'å­¦ç”Ÿ', 'é€€ä¼‘'],
                'mappings': {
                    'ä¸Šç­æ—': 'ä¼ä¸šå‘˜å·¥', 'å‘˜å·¥': 'ä¼ä¸šå‘˜å·¥', 'æ‰“å·¥äºº': 'ä¼ä¸šå‘˜å·¥', 'èŒå‘˜': 'ä¼ä¸šå‘˜å·¥',
                    'å…¬åŠ¡å‘˜': 'å…¬åŠ¡å‘˜', 'äº‹ä¸šå•ä½': 'å…¬åŠ¡å‘˜', 'å›½ä¼å‘˜å·¥': 'ä¼ä¸šå‘˜å·¥',
                    'ä¸ªä½“æˆ·': 'ä¸ªä½“ç»è¥', 'è‡ªè¥': 'ä¸ªä½“ç»è¥', 'å°è€æ¿': 'ä¸ªä½“ç»è¥',
                    'è€æ¿': 'ä¼ä¸šä¸»', 'åˆ›ä¸šè€…': 'ä¼ä¸šä¸»', 'ä¼ä¸šå®¶': 'ä¼ä¸šä¸»',
                    'è‡ªç”±èŒä¸šè€…': 'è‡ªç”±èŒä¸š', 'è‡ªç”±å·¥ä½œè€…': 'è‡ªç”±èŒä¸š', 'è‡ªé›‡': 'è‡ªç”±èŒä¸š',
                    'å¤§å­¦ç”Ÿ': 'å­¦ç”Ÿ', 'ç ”ç©¶ç”Ÿ': 'å­¦ç”Ÿ', 'åšå£«ç”Ÿ': 'å­¦ç”Ÿ',
                    'é€€ä¼‘äººå‘˜': 'é€€ä¼‘', 'å·²é€€ä¼‘': 'é€€ä¼‘'
                }
            },
            'family_structure': {
                'valid_values': ['å•èº«', 'å¤«å¦»', 'å¤«å¦»+å­å¥³', 'ä¸‰ä»£åŒå ‚', 'å…¶ä»–'],
                'mappings': {
                    'ä¸€ä¸ªäºº': 'å•èº«', 'ç‹¬å±…': 'å•èº«', 'å•èº«è´µæ—': 'å•èº«',
                    'ä¸¤å£å­': 'å¤«å¦»', 'å¤«å¦»äºŒäºº': 'å¤«å¦»', 'ä¸å…‹': 'å¤«å¦»',
                    'ä¸‰å£ä¹‹å®¶': 'å¤«å¦»+å­å¥³', 'å››å£ä¹‹å®¶': 'å¤«å¦»+å­å¥³', 'æœ‰å­©å­': 'å¤«å¦»+å­å¥³',
                    'å¤§å®¶åº­': 'ä¸‰ä»£åŒå ‚', 'å’Œçˆ¶æ¯ä¸€èµ·ä½': 'ä¸‰ä»£åŒå ‚'
                }
            },
            'income_stability': {
                'valid_values': ['éå¸¸ç¨³å®š', 'æ¯”è¾ƒç¨³å®š', 'ä¸€èˆ¬', 'ä¸ç¨³å®š'],
                'mappings': {
                    'å¾ˆç¨³å®š': 'éå¸¸ç¨³å®š', 'ç¨³å®š': 'æ¯”è¾ƒç¨³å®š', 'è¿˜è¡Œ': 'ä¸€èˆ¬',
                    'ä¸å¤ªç¨³å®š': 'ä¸ç¨³å®š', 'æ³¢åŠ¨å¤§': 'ä¸ç¨³å®š'
                }
            },
            'overall_health_status': {
                'valid_values': ['éå¸¸å¥åº·', 'æ¯”è¾ƒå¥åº·', 'ä¸€èˆ¬', 'æœ‰æ…¢æ€§ç—…', 'å¥åº·çŠ¶å†µè¾ƒå·®'],
                'mappings': {
                    'å¾ˆå¥åº·': 'éå¸¸å¥åº·', 'å¥åº·': 'æ¯”è¾ƒå¥åº·', 'è¿˜å¯ä»¥': 'ä¸€èˆ¬',
                    'æœ‰ç—…': 'æœ‰æ…¢æ€§ç—…', 'ä¸å¤ªå¥½': 'å¥åº·çŠ¶å†µè¾ƒå·®', 'è¾ƒå·®': 'å¥åº·çŠ¶å†µè¾ƒå·®'
                }
            },
            'has_chronic_disease': {
                'valid_values': ['æ— ', 'é«˜è¡€å‹', 'ç³–å°¿ç—…', 'å¿ƒè„ç—…', 'å…¶ä»–'],
                'mappings': {
                    'æ²¡æœ‰': 'æ— ', 'æ— æ…¢æ€§ç—…': 'æ— ', 'å¥åº·': 'æ— ',
                    'é«˜è¡€å‹ç—…': 'é«˜è¡€å‹', 'è¡€å‹é«˜': 'é«˜è¡€å‹',
                    'ç³–å°¿ç—…': 'ç³–å°¿ç—…', 'è¡€ç³–é«˜': 'ç³–å°¿ç—…',
                    'å¿ƒè„ç—…': 'å¿ƒè„ç—…', 'å¿ƒè¡€ç®¡ç–¾ç—…': 'å¿ƒè„ç—…'
                }
            },
            'smoking_status': {
                'valid_values': ['ä¸å¸çƒŸ', 'å·²æˆ’çƒŸ', 'è½»åº¦å¸çƒŸ', 'é‡åº¦å¸çƒŸ'],
                'mappings': {
                    'ä¸æŠ½çƒŸ': 'ä¸å¸çƒŸ', 'ä»ä¸å¸çƒŸ': 'ä¸å¸çƒŸ',
                    'æˆ’çƒŸäº†': 'å·²æˆ’çƒŸ', 'å·²ç»æˆ’äº†': 'å·²æˆ’çƒŸ',
                    'å¶å°”æŠ½': 'è½»åº¦å¸çƒŸ', 'å°‘é‡å¸çƒŸ': 'è½»åº¦å¸çƒŸ',
                    'ç»å¸¸æŠ½': 'é‡åº¦å¸çƒŸ', 'å¤§é‡å¸çƒŸ': 'é‡åº¦å¸çƒŸ'
                }
            },
            'is_family_financial_support': {
                'valid_values': ['æ˜¯', 'å¦', 'å…±åŒæ‰¿æ‹…'],
                'mappings': {
                    'æ˜¯çš„': 'æ˜¯', 'ä¸»è¦æ”¶å…¥æ¥æº': 'æ˜¯', 'ç»æµæ”¯æŸ±': 'æ˜¯',
                    'ä¸æ˜¯': 'å¦', 'ä¸ç®—': 'å¦',
                    'ä¸€èµ·æ‰¿æ‹…': 'å…±åŒæ‰¿æ‹…', 'å…±åŒè´Ÿæ‹…': 'å…±åŒæ‰¿æ‹…', 'éƒ½æœ‰æ”¶å…¥': 'å…±åŒæ‰¿æ‹…'
                }
            }
        }

    def _build_workflow(self) -> CompiledStateGraph[ProfileAnalyzerState, ProfileAnalyzerState, ProfileAnalyzerState]:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        workflow = StateGraph(ProfileAnalyzerState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("load_existing_features",
                          self._load_existing_features)
        workflow.add_node("analyze_conversation", self._analyze_conversation)
        workflow.add_node("extract_new_features", self._extract_new_features)
        workflow.add_node("update_database", self._update_database)
        workflow.add_node("evaluate_completeness", self._evaluate_completeness)
        workflow.add_node("generate_questions", self._generate_questions)
        workflow.add_node("finalize_complete", self._finalize_complete)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("load_existing_features")

        # æ·»åŠ è¾¹
        workflow.add_edge("load_existing_features", "analyze_conversation")
        workflow.add_edge("analyze_conversation", "extract_new_features")
        workflow.add_edge("extract_new_features", "update_database")
        workflow.add_edge("update_database", "evaluate_completeness")

        # æ¡ä»¶è¾¹ï¼šæ ¹æ®å®ŒæˆçŠ¶æ€å†³å®šä¸‹ä¸€æ­¥
        workflow.add_conditional_edges(
            "evaluate_completeness",
            self._decide_next_step,
            {
                "generate_questions": "generate_questions",
                "complete": "finalize_complete"
            }
        )

        workflow.add_edge("generate_questions", END)
        workflow.add_edge("finalize_complete", END)

        return workflow.compile()

    async def _load_existing_features(self, state: ProfileAnalyzerState) -> ProfileAnalyzerState:
        """åŠ è½½å·²æœ‰çš„ç”¨æˆ·ç‰¹å¾"""
        logger.info("å¼€å§‹åŠ è½½å·²æœ‰ç”¨æˆ·ç‰¹å¾")

        user_id = state["user_id"]
        session_id = state["session_id"]

        try:
            # ä»æ•°æ®åº“åŠ è½½å·²æœ‰ç‰¹å¾
            existing_features = await db_manager.get_user_features(user_id, session_id)

            # ç»„ç»‡ç‰¹å¾æ•°æ®
            organized_features: UserFeatures = {}
            for feature in existing_features:
                feature_name = feature['feature_name']

                organized_features[feature_name] = {
                    'category_name': feature['category_name'],
                    'feature_name': feature_name,
                    'feature_value': feature['feature_value'],
                    'confidence': feature['confidence'],
                    'skipped': feature['skipped']
                }

            state["existing_features"] = organized_features
            logger.info(f"åŠ è½½äº†ç”¨æˆ· {user_id} çš„ {len(existing_features)} ä¸ªå·²æœ‰ç‰¹å¾")

        except Exception as e:
            logger.error(f"åŠ è½½ç”¨æˆ·ç‰¹å¾å¤±è´¥: {e}")
            state["existing_features"] = {}

        return state

    async def _analyze_conversation(self, state: ProfileAnalyzerState) -> ProfileAnalyzerState:
        """åˆ†æå¯¹è¯å†…å®¹"""
        logger.info("å¼€å§‹åˆ†æå¯¹è¯å†…å®¹")

        history_chats = state.get("history_chats", [])

        if history_chats:
            try:
                # æ·»åŠ å¯¹è¯åˆ°è®°å¿†ç®¡ç†å™¨
                await memory_manager.add_conversation_memory(
                    user_id=state["user_id"],
                    conversation=history_chats
                )
                logger.info("å¯¹è¯å·²æ·»åŠ åˆ°è®°å¿†ç®¡ç†å™¨")

            except Exception as e:
                logger.error(f"æ·»åŠ å¯¹è¯è®°å¿†å¤±è´¥: {e}")

        return state

    async def _extract_new_features(self, state: ProfileAnalyzerState) -> ProfileAnalyzerState:
        """æå–æ–°ç‰¹å¾"""
        logger.info("å¼€å§‹æå–æ–°ç‰¹å¾")

        history_chats = state.get("history_chats", [])
        custom_profile = state.get("custom_profile", {})
        existing_features = state.get("existing_features", {})

        extracted_features: UserFeatures = {}

        try:
            # 1. å¤„ç† custom_profileï¼ˆç½®ä¿¡åº¦ 1.0ï¼‰
            if custom_profile:
                custom_features = self._process_custom_profile(custom_profile)
                extracted_features.update(custom_features)

            # 2. ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½çš„ç‰¹å¾æå–
            if history_chats:
                llm_features = await self._extract_features_with_llm(history_chats, existing_features)

                # åˆå¹¶ LLM æå–çš„ç‰¹å¾
                for feature_name, feature_data in llm_features.items():
                    # å¦‚æœç‰¹å¾å·²å­˜åœ¨ï¼Œé€‰æ‹©ç½®ä¿¡åº¦æ›´é«˜çš„
                    if feature_name in extracted_features:
                        if feature_data['confidence'] > extracted_features[feature_name]['confidence']:
                            extracted_features[feature_name] = feature_data
                    else:
                        extracted_features[feature_name] = feature_data

            state["extracted_features"] = extracted_features
            total_features = len(extracted_features)
            logger.info(f"æå–äº† {total_features} ä¸ªæ–°ç‰¹å¾")

        except Exception as e:
            logger.error(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            state["extracted_features"] = {}

        return state

    async def _update_database(self, state: ProfileAnalyzerState) -> ProfileAnalyzerState:
        """æ›´æ–°æ•°æ®åº“"""
        logger.info("å¼€å§‹æ›´æ–°æ•°æ®åº“")

        user_id = state["user_id"]
        session_id = state["session_id"]
        extracted_features = state.get("extracted_features", {})

        try:
            # æ›´æ–°æ•°æ®åº“ä¸­çš„ç‰¹å¾
            for feature_name, feature_data in extracted_features.items():
                await db_manager.upsert_user_feature(
                    user_id=user_id,
                    session_id=session_id,
                    category_name=feature_data['category_name'],
                    feature_name=feature_name,
                    feature_value=feature_data['feature_value'],
                    confidence=feature_data['confidence'],
                    skipped=feature_data.get('skipped', False)
                )

            logger.info(f"æˆåŠŸæ›´æ–°ç”¨æˆ· {user_id} çš„ç‰¹å¾åˆ°æ•°æ®åº“")

        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®åº“å¤±è´¥: {e}")

        return state

    async def _evaluate_completeness(self, state: ProfileAnalyzerState) -> ProfileAnalyzerState:
        """è¯„ä¼°å®Œæ•´æ€§å’Œå®ŒæˆçŠ¶æ€"""
        logger.info("å¼€å§‹è¯„ä¼°å®Œæ•´æ€§å’Œå®ŒæˆçŠ¶æ€")

        user_id = state["user_id"]
        session_id = state["session_id"]

        try:
            # è·å–æœ€æ–°çš„ç”¨æˆ·ç‰¹å¾æ‘˜è¦
            profile_summary = await db_manager.get_user_profile_summary(user_id, session_id)

            # åˆ†æç‰¹å¾è¦†ç›–æƒ…å†µ
            coverage_analysis = self._analyze_feature_coverage(
                profile_summary['profile'])

            # æ£€æŸ¥å¿…è¦ç‰¹å¾å®Œæ•´æ€§
            required_completeness = self._check_required_features(
                profile_summary['profile'])

            completion_status = {
                'coverage_analysis': coverage_analysis,
                'required_completeness': required_completeness,
                'profile_summary': profile_summary,
                'is_complete': coverage_analysis['all_covered'] and required_completeness['all_required_complete']
            }

            state["completion_status"] = completion_status
            state["is_complete"] = completion_status['is_complete']

            logger.info(
                f"å®Œæ•´æ€§è¯„ä¼°å®Œæˆ: è¦†ç›–ç‡ {coverage_analysis['coverage_rate']:.2f}, å¿…è¦ç‰¹å¾å®Œæ•´æ€§ {required_completeness['completion_rate']:.2f}")

        except Exception as e:
            logger.error(f"è¯„ä¼°å®Œæ•´æ€§å¤±è´¥: {e}")
            state["completion_status"] = {
                'coverage_analysis': {'all_covered': False, 'coverage_rate': 0.0},
                'required_completeness': {'all_required_complete': False, 'completion_rate': 0.0},
                'is_complete': False
            }
            state["is_complete"] = False

        return state

    async def _generate_questions(self, state: ProfileAnalyzerState) -> ProfileAnalyzerState:
        """ç”Ÿæˆé—®é¢˜"""
        logger.info("å¼€å§‹ç”Ÿæˆé—®é¢˜")

        completion_status = state.get("completion_status", {})
        coverage_analysis = completion_status.get('coverage_analysis', {})
        required_completeness = completion_status.get(
            'required_completeness', {})

        try:
            # ç¡®å®šéœ€è¦è¯¢é—®çš„ç‰¹å¾
            missing_features = coverage_analysis.get('missing_features', [])
            missing_required = required_completeness.get(
                'missing_required', [])

            # ç”Ÿæˆé—®é¢˜
            questions = await self._generate_smart_questions(missing_features, missing_required, state)

            state["suggested_questions"] = questions
            state["final_answer"] = questions

        except Exception as e:
            logger.error(f"ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
            state["suggested_questions"] = "æŠ±æ­‰ï¼Œç”Ÿæˆé—®é¢˜æ—¶å‡ºç°é”™è¯¯ã€‚"
            state["final_answer"] = "æŠ±æ­‰ï¼Œç”Ÿæˆé—®é¢˜æ—¶å‡ºç°é”™è¯¯ã€‚"

        return state

    async def _finalize_complete(self, state: ProfileAnalyzerState) -> ProfileAnalyzerState:
        """å®Œæˆåˆ†æ"""
        logger.info("ç”¨æˆ·ç”»åƒåˆ†æå®Œæˆ")

        completion_status = state.get("completion_status", {})
        profile_summary = completion_status.get('profile_summary', {})

        # ç”Ÿæˆå®Œæˆå“åº”
        final_answer = f"""
        ğŸ‰ æ­å–œï¼æ‚¨çš„ç”¨æˆ·ç”»åƒåˆ†æå·²å®Œæˆï¼

        ## ç”»åƒå®Œæ•´åº¦
        - æ€»ä½“å®Œæˆç‡: {profile_summary.get('completion_rate', 0) * 100:.1f}%
        - å·²å®Œæˆç‰¹å¾: {profile_summary.get('completed_features', 0)} ä¸ª
        - æ€»ç‰¹å¾æ•°: {profile_summary.get('total_features', 0)} ä¸ª

        ## ä¸‹ä¸€æ­¥
        åŸºäºæ‚¨å®Œæ•´çš„ç”¨æˆ·ç”»åƒï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ä¸ºæ‚¨æ¨èæœ€é€‚åˆçš„ä¿é™©äº§å“ã€‚
        æ‚¨å¯ä»¥ç»§ç»­ä½¿ç”¨äº§å“æ¨èåŠŸèƒ½æ¥è·å–ä¸ªæ€§åŒ–çš„ä¿é™©å»ºè®®ã€‚
        """

        state["final_answer"] = final_answer
        state["is_complete"] = True

        return state

    def _decide_next_step(self, state: ProfileAnalyzerState) -> str:
        """å†³å®šä¸‹ä¸€æ­¥æ“ä½œ"""
        is_complete = state.get("is_complete", False)

        if is_complete:
            return "complete"
        else:
            return "generate_questions"

    # è¾…åŠ©æ–¹æ³•
    async def _call_llm_with_logging(
        self,
        messages: List[BaseMessage],
        operation_name: str
    ) -> str:
        """è°ƒç”¨ LLM å¹¶è®°å½•è¯¦ç»†æ—¥å¿—"""
        try:
            # è®°å½• prompt å‰100ä¸ªå­—ç¬¦
            prompt_preview = ""
            if messages:
                first_message = messages[0]
                if hasattr(first_message, 'content') and first_message.content:
                    prompt_preview = str(first_message.content)[:100]

            logger.info(f"å¼€å§‹ LLM è°ƒç”¨ - {operation_name}")
            logger.info(f"Prompt é¢„è§ˆ: {prompt_preview}...")

            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()

            # è°ƒç”¨ LLM
            response = await self.llm.ainvoke(messages)

            # è®°å½•ç»“æŸæ—¶é—´å’Œè€—æ—¶
            end_time = time.time()
            duration = end_time - start_time

            # è®°å½•å“åº”ä¿¡æ¯
            response_content = str(
                response.content) if response.content else ""

            # è®°å½• token ä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœå¯ç”¨ï¼‰
            token_info = ""
            if hasattr(response, 'response_metadata') and response.response_metadata:
                usage = response.response_metadata.get('token_usage', {})
                if usage:
                    prompt_tokens = usage.get('prompt_tokens', 0)
                    completion_tokens = usage.get('completion_tokens', 0)
                    total_tokens = usage.get('total_tokens', 0)
                    token_info = f"Tokens - Prompt: {prompt_tokens}, Completion: {completion_tokens}, Total: {total_tokens}"

            logger.info(f"LLM è°ƒç”¨å®Œæˆ - {operation_name}")
            logger.info(f"è€—æ—¶: {duration:.2f}ç§’")
            if token_info:
                logger.info(token_info)
            logger.info(f"å“åº”é•¿åº¦: {len(response_content)} å­—ç¬¦")

            return response_content

        except Exception as e:
            logger.error(f"LLM è°ƒç”¨å¤±è´¥ - {operation_name}: {e}")
            raise

    def _process_custom_profile(self, custom_profile: UserProfile) -> UserFeatures:
        """å¤„ç†è‡ªå®šä¹‰ç”»åƒï¼ˆç½®ä¿¡åº¦ 1.0ï¼‰"""
        processed_features: UserFeatures = {}

        for feature_name, value in custom_profile.items():
            if value is None:
                continue

            # ç¡®å®šç‰¹å¾æ‰€å±åˆ†ç±»
            category = self._get_feature_category(feature_name)
            if category:
                # æ ‡å‡†åŒ–å­—æ®µå€¼
                normalized_value = self._normalize_field_value(
                    feature_name, value)

                processed_features[feature_name] = {
                    'category_name': category,
                    'feature_name': feature_name,
                    'feature_value': normalized_value,
                    'confidence': 1.0,
                    'skipped': False
                }

        return processed_features

    def _get_feature_category(self, feature_name: str) -> Optional[str]:
        """è·å–ç‰¹å¾æ‰€å±åˆ†ç±»"""
        for category, features in self.feature_categories.items():
            if feature_name in features:
                return category
        return None

    def _normalize_field_value(self, feature_name: str, value: Any) -> Any:
        """æ ‡å‡†åŒ–å­—æ®µå€¼ï¼Œç¡®ä¿ç¬¦åˆé¢„æœŸçš„æšä¸¾å€¼"""
        if not isinstance(value, str):
            return value

        # æ¸…ç†è¾“å…¥å€¼
        cleaned_value = str(value).strip()

        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡å‡†åŒ–è§„åˆ™
        if feature_name not in self.field_normalization_rules:
            return cleaned_value

        rules = self.field_normalization_rules[feature_name]

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æœ‰æ•ˆå€¼
        if cleaned_value in rules['valid_values']:
            return cleaned_value

        # å°è¯•æ˜ å°„è½¬æ¢
        if cleaned_value in rules['mappings']:
            normalized_value = rules['mappings'][cleaned_value]
            logger.info(
                f"å­—æ®µ {feature_name} å€¼æ ‡å‡†åŒ–: '{cleaned_value}' -> '{normalized_value}'")
            return normalized_value

        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
        cleaned_lower = cleaned_value.lower()
        for mapping_key, mapping_value in rules['mappings'].items():
            if mapping_key.lower() == cleaned_lower:
                logger.info(
                    f"å­—æ®µ {feature_name} å€¼æ ‡å‡†åŒ–(å¿½ç•¥å¤§å°å†™): '{cleaned_value}' -> '{mapping_value}'")
                return mapping_value

        # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›åŸå€¼
        logger.warning(
            f"å­—æ®µ {feature_name} çš„å€¼ '{cleaned_value}' ä¸åœ¨é¢„æœŸçš„æšä¸¾å€¼ä¸­: {rules['valid_values']}")
        return cleaned_value

    async def _extract_features_with_llm(
        self,
        history_chats: List[ChatMessage],
        existing_features: UserFeatures
    ) -> UserFeatures:
        """ä½¿ç”¨ LLM æå–ç‰¹å¾"""
        try:
            # æ„å»ºå¯¹è¯æ–‡æœ¬
            conversation_text = "\n".join([
                f"{chat['role']}: {chat['content']}"
                for chat in history_chats
            ])

            # æ„å»ºæç¤ºè¯ï¼Œä¼ å…¥å·²æœ‰ç‰¹å¾
            system_prompt = self._build_feature_extraction_prompt(
                existing_features)

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"è¯·åˆ†æä»¥ä¸‹å¯¹è¯å¹¶æå–ç”¨æˆ·ç‰¹å¾ï¼š\n\n{conversation_text}")
            ]

            # ä½¿ç”¨å¸¦æ—¥å¿—çš„ LLM è°ƒç”¨
            response_content = await self._call_llm_with_logging(messages, "ç‰¹å¾æå–")

            # è§£æ LLM å“åº”
            if response_content:
                return self._parse_llm_feature_response(response_content)
            else:
                return {}

        except Exception as e:
            logger.error(f"LLM ç‰¹å¾æå–å¤±è´¥: {e}")
            return {}

    def _build_feature_extraction_prompt(self, existing_features: UserFeatures) -> str:
        """æ„å»ºç‰¹å¾æå–æç¤ºè¯"""

        # å‡†å¤‡å·²æœ‰ç‰¹å¾ä¿¡æ¯
        existing_features_text = self._format_existing_features_for_llm(
            existing_features)

        return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©ç»çºªäººå’Œç”¨æˆ·ç”»åƒåˆ†æå¸ˆã€‚è¯·ä»ç”¨æˆ·çš„å¯¹è¯ä¸­æå–ä»¥ä¸‹ç‰¹å¾ä¿¡æ¯ï¼š

{self._format_feature_categories()}

å·²æœ‰çš„ç”¨æˆ·ç‰¹å¾ä¿¡æ¯ï¼š
{existing_features_text}

è¯·ä»¥ JSON æ ¼å¼è¿”å›æå–çš„ç‰¹å¾ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "category_name": {{
        "feature_name": {{
            "value": "ç‰¹å¾å€¼",
            "confidence": 0.8,
            "reasoning": "æå–ç†ç”±"
        }}
    }}
}}

æ³¨æ„äº‹é¡¹ï¼š
1. åªæå–å¯¹è¯ä¸­æ˜ç¡®æåˆ°æˆ–å¯ä»¥åˆç†æ¨æ–­çš„ä¿¡æ¯
2. ç½®ä¿¡åº¦èŒƒå›´ 0.0-1.0ï¼Œç›´æ¥æåˆ°çš„ä¿¡æ¯ç½®ä¿¡åº¦ 0.8ï¼Œæ¨æ–­çš„ä¿¡æ¯ç½®ä¿¡åº¦ 0.5
3. å¦‚æœæŸä¸ªç‰¹å¾æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ä¸è¦åŒ…å«åœ¨ç»“æœä¸­
4. å¥³æ€§ç‰¹æ®ŠçŠ¶æ€åªæœ‰åœ¨ç¡®è®¤ç”¨æˆ·æ˜¯å¥³æ€§æ—¶æ‰æå–
5. å¦‚æœå·²æœ‰ç‰¹å¾ä¿¡æ¯ä¸å¯¹è¯å†…å®¹å†²çªï¼Œè¯·åœ¨ reasoning ä¸­è¯´æ˜
6. ä¸è¦é‡å¤æå–å·²æœ‰çš„ç‰¹å¾ï¼Œé™¤éå‘ç°äº†æ›´å‡†ç¡®çš„ä¿¡æ¯
"""

    def _format_feature_categories(self) -> str:
        """æ ¼å¼åŒ–ç‰¹å¾åˆ†ç±»ä¿¡æ¯ï¼ŒåŒ…å«è¯¦ç»†çš„ç‰¹å¾å®šä¹‰"""

        # è¯¦ç»†çš„ç‰¹å¾å®šä¹‰ï¼Œæ¥è‡ªè®¾è®¡æ–‡æ¡£
        feature_definitions = {
            "basic_identity": {
                "gender": "æ€§åˆ« (ç”·/å¥³) - äº§å“åŒ¹é…ã€è´¹ç‡è®¡ç®—",
                "date_of_birth": "å‡ºç”Ÿå¹´æœˆæ—¥ (YYYY-MM-DD) - ç²¾ç¡®å¹´é¾„è®¡ç®—ã€äº§å“é€‚é…ã€è´¹ç‡è®¡ç®—",
                "marital_status": "å©šå§»çŠ¶å†µ (æœªå©š/å·²å©š/ç¦»å¼‚/ä¸§å¶) - å®¶åº­è´£ä»»è¯„ä¼°",
                "residence_city": "å¸¸ä½åŸå¸‚ - äº§å“åœ°åŒºé™åˆ¶ã€åŒ»ç–—èµ„æºè¯„ä¼°",
                "occupation_type": "èŒä¸šç±»å‹ (ä¼ä¸šå‘˜å·¥/å…¬åŠ¡å‘˜/ä¸ªä½“ç»è¥/ä¼ä¸šä¸»/è‡ªç”±èŒä¸š/å­¦ç”Ÿ/é€€ä¼‘) - èŒä¸šé£é™©è¯„ä¼°ã€äº§å“åŒ¹é…",
                "industry": "æ‰€å±è¡Œä¸š - èŒä¸šé£é™©è¯„ä¼°ã€æ”¶å…¥ç¨³å®šæ€§åˆ¤æ–­"
            },
            "female_specific": {
                "pregnancy_status": "å­•æœŸçŠ¶æ€ (æœªæ€€å­•/å¤‡å­•æœŸ/å­•æ—©æœŸ(1-3æœˆ)/å­•ä¸­æœŸ(4-6æœˆ)/å­•æ™šæœŸ(7-9æœˆ)/äº§åæ¢å¤æœŸ) - æ¯å©´ä¿é™©éœ€æ±‚è¯„ä¼°ã€è´¹ç‡è®¡ç®—",
                "childbearing_plan": "ç”Ÿè‚²è®¡åˆ’ (1å¹´å†…/2å¹´å†…/3å¹´å†…/æš‚æ— è®¡åˆ’) - æ¯å©´ä¿éšœè§„åˆ’ã€äº§å“æ¨è"
            },
            "family_structure": {
                "family_structure": "å®¶åº­ç»“æ„ (å•èº«/å¤«å¦»/å¤«å¦»+å­å¥³/ä¸‰ä»£åŒå ‚/å…¶ä»–) - å®¶åº­è´£ä»»è¯„ä¼°åŸºç¡€",
                "number_of_children": "å­å¥³æ•°é‡ (æ•´æ•°) - æ•™è‚²è§„åˆ’éœ€æ±‚ã€å®¶åº­è´£ä»»è®¡ç®—",
                "caregiving_responsibility": "èµ¡å…»è´£ä»» (æ— /èµ¡å…»çˆ¶æ¯/èµ¡å…»åŒæ–¹çˆ¶æ¯/å…¶ä»–) - å®¶åº­è´£ä»»è¯„ä¼°",
                "monthly_household_expense": "æœˆåº¦å®¶åº­æ”¯å‡º (å…ƒï¼Œæ•°å­—) - å®¶åº­è´£ä»»è®¡ç®—ã€å¯¿é™©ä¿é¢è®¡ç®—",
                "mortgage_balance": "æˆ¿è´·ä½™é¢ (å…ƒï¼Œæ•°å­—) - å®¶åº­è´£ä»»è®¡ç®—ã€å¯¿é™©ä¿é¢è®¡ç®—",
                "is_family_financial_support": "æ˜¯å¦å®¶åº­ç»æµæ”¯æŸ± (æ˜¯/å¦/å…±åŒæ‰¿æ‹…) - å®¶åº­è´£ä»»è¯„ä¼°ã€ä¿é¢è®¡ç®—"
            },
            "financial_status": {
                "annual_total_income": "å¹´æ€»æ”¶å…¥ (ä¸‡å…ƒï¼Œæ•°å­—) - ä¿é¢è®¡ç®—ã€é¢„ç®—è§„åˆ’",
                "income_stability": "æ”¶å…¥ç¨³å®šæ€§ (éå¸¸ç¨³å®š/æ¯”è¾ƒç¨³å®š/ä¸€èˆ¬/ä¸ç¨³å®š) - é£é™©è¯„ä¼°ã€äº§å“é€‰æ‹©",
                "annual_insurance_budget": "å¹´åº¦ä¿é™©é¢„ç®— (ä¸‡å…ƒï¼Œæ•°å­—) - äº§å“ç»„åˆè§„åˆ’ã€ä¿è´¹é¢„ç®—åˆ†é…"
            },
            "health_lifestyle": {
                "overall_health_status": "æ•´ä½“å¥åº·çŠ¶å†µ (éå¸¸å¥åº·/æ¯”è¾ƒå¥åº·/ä¸€èˆ¬/æœ‰æ…¢æ€§ç—…/å¥åº·çŠ¶å†µè¾ƒå·®) - äº§å“åŒ¹é…ã€è´¹ç‡è®¡ç®—",
                "has_chronic_disease": "æ˜¯å¦æœ‰æ…¢æ€§ç–¾ç—… (æ— /é«˜è¡€å‹/ç³–å°¿ç—…/å¿ƒè„ç—…/å…¶ä»–) - æ‰¿ä¿è¯„ä¼°ã€äº§å“åŒ¹é…",
                "smoking_status": "å¸çƒŸæƒ…å†µ (ä¸å¸çƒŸ/å·²æˆ’çƒŸ/è½»åº¦å¸çƒŸ/é‡åº¦å¸çƒŸ) - å¥åº·é£é™©è¯„ä¼°ã€è´¹ç‡è®¡ç®—",
                "recent_medical_checkup": "è¿‘æœŸä½“æ£€æƒ…å†µ (1å¹´å†…æ­£å¸¸/1å¹´å†…æœ‰å¼‚å¸¸/2å¹´å†…æ­£å¸¸/2å¹´ä»¥ä¸Šæœªä½“æ£€) - å¥åº·é£é™©è¯„ä¼°ã€æ‰¿ä¿è¯„ä¼°"
            }
        }

        formatted = []
        for category, features in self.feature_categories.items():
            # æ·»åŠ åˆ†ç±»æ ‡é¢˜
            category_title = {
                "basic_identity": "åŸºç¡€èº«ä»½ç»´åº¦ (basic_identity)",
                "female_specific": "å¥³æ€§ç‰¹æ®ŠçŠ¶æ€ (female_specific) - ä»…å¥³æ€§",
                "family_structure": "å®¶åº­ç»“æ„ä¸è´£ä»»ç»´åº¦ (family_structure)",
                "financial_status": "è´¢åŠ¡ç°çŠ¶ä¸ç›®æ ‡ç»´åº¦ (financial_status)",
                "health_lifestyle": "å¥åº·ä¸ç”Ÿæ´»ä¹ æƒ¯ç»´åº¦ (health_lifestyle)"
            }.get(category, category)

            formatted.append(f"\n{category_title}:")

            # æ·»åŠ ç‰¹å¾å®šä¹‰
            for feature in features:
                definition = feature_definitions.get(
                    category, {}).get(feature, feature)
                formatted.append(f"  - {feature}: {definition}")

        return "\n".join(formatted)

    def _format_existing_features_for_llm(self, existing_features: UserFeatures) -> str:
        """æ ¼å¼åŒ–å·²æœ‰ç‰¹å¾ä¿¡æ¯ä¾› LLM ä½¿ç”¨"""
        if not existing_features:
            return "æš‚æ— å·²æœ‰ç‰¹å¾ä¿¡æ¯"

        formatted_features: List[ExistingFeatureForLLM] = []

        for feature_name, feature_data in existing_features.items():
            # è·³è¿‡å·²è·³è¿‡çš„ç‰¹å¾
            if not feature_data.get('skipped', False):
                formatted_features.append({
                    'category_name': feature_data['category_name'],
                    'feature_name': feature_name,
                    'feature_value': feature_data['feature_value'],
                    'confidence': feature_data['confidence']
                })

        if not formatted_features:
            return "æš‚æ— æœ‰æ•ˆçš„å·²æœ‰ç‰¹å¾ä¿¡æ¯"

        # æ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æ–‡æœ¬
        formatted_lines = []
        for feature in formatted_features:
            formatted_lines.append(
                f"- {feature['category_name']}.{feature['feature_name']}: "
                f"{feature['feature_value']} (ç½®ä¿¡åº¦: {feature['confidence']:.2f})"
            )

        return "\n".join(formatted_lines)

    def _parse_llm_feature_response(self, response_content: str) -> UserFeatures:
        """è§£æ LLM çš„ç‰¹å¾æå–å“åº”"""
        try:
            import json
            import re

            # å°è¯•æå– JSON å†…å®¹
            json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
            if not json_match:
                logger.warning("LLM å“åº”ä¸­æœªæ‰¾åˆ° JSON æ ¼å¼çš„å†…å®¹")
                return {}

            json_str = json_match.group(0)
            parsed_data = json.loads(json_str)

            # è½¬æ¢ä¸º UserFeatures æ ¼å¼
            result: UserFeatures = {}

            for category_name, features in parsed_data.items():
                if category_name in self.feature_categories:
                    for feature_name, feature_info in features.items():
                        if feature_name in self.feature_categories[category_name]:
                            if isinstance(feature_info, dict) and 'value' in feature_info:
                                # æ ‡å‡†åŒ–å­—æ®µå€¼
                                normalized_value = self._normalize_field_value(
                                    feature_name, feature_info['value'])

                                result[feature_name] = {
                                    'category_name': category_name,
                                    'feature_name': feature_name,
                                    'feature_value': normalized_value,
                                    'confidence': feature_info.get('confidence', 0.5),
                                    'skipped': False
                                }

            return result

        except Exception as e:
            logger.error(f"è§£æ LLM å“åº”å¤±è´¥: {e}")
            logger.debug(f"LLM å“åº”å†…å®¹: {response_content}")
            return {}

    def _analyze_feature_coverage(self, profile: UserFeatures) -> Dict[str, Any]:
        """åˆ†æç‰¹å¾è¦†ç›–æƒ…å†µ"""
        total_features = sum(len(features)
                             for features in self.feature_categories.values())
        covered_features = 0
        missing_features = []

        for category, expected_features in self.feature_categories.items():
            for feature_name in expected_features:
                if feature_name in profile:
                    feature_data = profile[feature_name]
                    # å¦‚æœç‰¹å¾æœ‰å€¼æˆ–è¢«è·³è¿‡ï¼Œéƒ½ç®—ä½œå·²è¦†ç›–
                    if feature_data.get('skipped') or (feature_data.get('feature_value') is not None and feature_data.get('confidence', 0) > 0):
                        covered_features += 1
                    else:
                        missing_features.append((category, feature_name))
                else:
                    missing_features.append((category, feature_name))

        coverage_rate = covered_features / total_features if total_features > 0 else 0.0

        return {
            'total_features': total_features,
            'covered_features': covered_features,
            'missing_features': missing_features,
            'coverage_rate': coverage_rate,
            'all_covered': coverage_rate >= 0.8  # 80% è¦†ç›–ç‡è®¤ä¸ºå·²å®Œæˆ
        }

    def _check_required_features(self, profile: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æŸ¥å¿…è¦ç‰¹å¾å®Œæ•´æ€§

        Args:
            profile: æ‰å¹³æ ¼å¼çš„ç‰¹å¾æ•°æ® (UserFeatures): {feature_name: {feature_value, confidence, skipped, ...}}
        """
        completed_required = 0
        missing_required = []

        for feature_name in self.required_features:
            if feature_name in profile:
                feature_data = profile[feature_name]
                # å¿…è¦ç‰¹å¾å¿…é¡»æœ‰å€¼ä¸”ç½®ä¿¡åº¦ > 0.5
                if (feature_data.get('feature_value') is not None and
                    feature_data.get('confidence', 0) > 0.5 and
                        not feature_data.get('skipped', False)):
                    completed_required += 1
                else:
                    missing_required.append(feature_name)
            else:
                missing_required.append(feature_name)

        total_required = len(self.required_features)
        completion_rate = completed_required / \
            total_required if total_required > 0 else 0.0

        return {
            'total_required': total_required,
            'completed_required': completed_required,
            'missing_required': missing_required,
            'completion_rate': completion_rate,
            'all_required_complete': completion_rate >= 1.0
        }

    def _get_user_questions_history(self, user_id: int) -> List[str]:
        """è·å–ç”¨æˆ·çš„å†å²é—®é¢˜è®°å½•"""
        return self.user_questions_history.get(user_id, [])

    def _get_user_features_history(self, user_id: int) -> List[str]:
        """è·å–ç”¨æˆ·çš„å†å²ç‰¹å¾è®°å½•"""
        return self.user_features_history.get(user_id, [])

    def _add_user_question(self, user_id: int, question: str):
        """æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²è®°å½•"""
        if user_id not in self.user_questions_history:
            self.user_questions_history[user_id] = []
        self.user_questions_history[user_id].append(question)

        # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼Œé¿å…å†…å­˜è¿‡åº¦ä½¿ç”¨
        if len(self.user_questions_history[user_id]) > 10:
            self.user_questions_history[user_id] = self.user_questions_history[user_id][-10:]

    def _add_user_features(self, user_id: int, features: List[str]):
        """æ·»åŠ ç”¨æˆ·ç‰¹å¾åˆ°å†å²è®°å½•"""
        if user_id not in self.user_features_history:
            self.user_features_history[user_id] = []

        for feature in features:
            if feature not in self.user_features_history[user_id]:
                self.user_features_history[user_id].append(feature)

    def clear_user_history(self, user_id: int):
        """æ¸…ç©ºæŒ‡å®šç”¨æˆ·çš„å†å²è®°å½•"""
        if user_id in self.user_questions_history:
            del self.user_questions_history[user_id]
        if user_id in self.user_features_history:
            del self.user_features_history[user_id]
        logger.info(f"å·²æ¸…ç©ºç”¨æˆ· {user_id} çš„å†å²è®°å½•")

    async def _generate_smart_questions(
        self,
        missing_features: List[Tuple[str, str]],
        missing_required: List[str],
        state: ProfileAnalyzerState
    ) -> str:
        """ç”Ÿæˆæ™ºèƒ½é—®é¢˜ï¼Œé¿å…é‡å¤è¯¢é—®"""
        try:
            user_id = state["user_id"]

            # è·å–æŒä¹…åŒ–çš„å·²è¯¢é—®è¿‡çš„ç‰¹å¾
            asked_features = set(self._get_user_features_history(user_id))

            # è¿‡æ»¤æ‰å·²è¯¢é—®è¿‡çš„å¿…è¦ç‰¹å¾
            filtered_missing_required = [
                feature for feature in missing_required
                if feature not in asked_features
            ]

            # è¿‡æ»¤æ‰å·²è¯¢é—®è¿‡çš„å…¶ä»–ç‰¹å¾
            filtered_missing_features = [
                (category, feature) for category, feature in missing_features
                if feature not in asked_features
            ]

            # ä¼˜å…ˆè¯¢é—®æœªé—®è¿‡çš„å¿…è¦ç‰¹å¾
            priority_features = []
            if filtered_missing_required:
                # æœ€å¤šè¯¢é—®3ä¸ªå¿…è¦ç‰¹å¾
                for feature_name in filtered_missing_required[:3]:
                    category = self._get_feature_category(feature_name)
                    if category:
                        priority_features.append((category, feature_name))

            # å¦‚æœå¿…è¦ç‰¹å¾ä¸è¶³3ä¸ªï¼Œè¡¥å……å…¶ä»–æœªé—®è¿‡çš„ç¼ºå¤±ç‰¹å¾
            if len(priority_features) < 3:
                for category, feature_name in filtered_missing_features:
                    if (category, feature_name) not in priority_features:
                        priority_features.append((category, feature_name))
                        if len(priority_features) >= 3:
                            break

            # å¦‚æœæ‰€æœ‰ç‰¹å¾éƒ½é—®è¿‡äº†ï¼Œä½†ä»æœ‰ç¼ºå¤±ï¼Œåˆ™é‡æ–°è¯¢é—®æœ€é‡è¦çš„ç‰¹å¾
            if not priority_features and (missing_required or missing_features):
                logger.info("æ‰€æœ‰ç‰¹å¾éƒ½å·²è¯¢é—®è¿‡ï¼Œé‡æ–°è¯¢é—®æœ€é‡è¦çš„å¿…è¦ç‰¹å¾")
                if missing_required:
                    feature_name = missing_required[0]
                    category = self._get_feature_category(feature_name)
                    if category:
                        priority_features.append((category, feature_name))

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¯¢é—®å¥³æ€§ç‰¹æ®ŠçŠ¶æ€
            should_ask_female_specific = self._should_ask_female_specific(
                state)

            # ç”Ÿæˆé—®é¢˜
            questions_prompt = self._build_questions_prompt(
                priority_features, should_ask_female_specific, state)

            messages = [
                SystemMessage(content=questions_prompt),
                HumanMessage(content="è¯·ç”Ÿæˆè‡ªç„¶ã€ç›´æ¥çš„é—®é¢˜æ¥æ”¶é›†è¿™äº›ä¿¡æ¯ã€‚")
            ]

            # ä½¿ç”¨å¸¦æ—¥å¿—çš„ LLM è°ƒç”¨
            response_content = await self._call_llm_with_logging(messages, "é—®é¢˜ç”Ÿæˆ")

            # è®°å½•æœ¬æ¬¡è¯¢é—®çš„ç‰¹å¾å’Œé—®é¢˜åˆ°æŒä¹…åŒ–å­˜å‚¨
            if priority_features and response_content:
                features_to_record = [
                    f for _, f in priority_features if f not in asked_features]
                if features_to_record:
                    self._add_user_features(user_id, features_to_record)
                    logger.info(f"è®°å½•è¯¢é—®ç‰¹å¾: {features_to_record}")

                self._add_user_question(user_id, response_content)
                logger.info(f"è®°å½•è¯¢é—®é—®é¢˜: {response_content[:50]}...")

            return response_content if response_content else "è¯·å‘Šè¯‰æˆ‘æ›´å¤šå…³äºæ‚¨çš„ä¿¡æ¯ï¼Œä»¥ä¾¿ä¸ºæ‚¨æä¾›æ›´å¥½çš„ä¿é™©å»ºè®®ã€‚"

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ™ºèƒ½é—®é¢˜å¤±è´¥: {e}")
            return "è¯·å‘Šè¯‰æˆ‘æ›´å¤šå…³äºæ‚¨çš„ä¿¡æ¯ï¼Œä»¥ä¾¿ä¸ºæ‚¨æä¾›æ›´å¥½çš„ä¿é™©å»ºè®®ã€‚"

    def _should_ask_female_specific(self, state: ProfileAnalyzerState) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¯¢é—®å¥³æ€§ç‰¹æ®ŠçŠ¶æ€"""
        existing_features = state.get("existing_features", {})

        # ä»æ‰å¹³ç»“æ„ä¸­ç›´æ¥æŸ¥æ‰¾ gender ç‰¹å¾
        gender_data = existing_features.get("gender", {})

        if gender_data:
            gender_value = gender_data.get("feature_value", "")
            if isinstance(gender_value, str):
                gender_value = gender_value.lower()
            confidence = gender_data.get("confidence", 0)
            return gender_value == "å¥³" and confidence > 0.5

        return False

    def _build_questions_prompt(self, priority_features: List[Tuple[str, str]], ask_female_specific: bool, state: ProfileAnalyzerState) -> str:
        """æ„å»ºé—®é¢˜ç”Ÿæˆæç¤ºè¯ï¼Œé¿å…é‡å¤è¯¢é—®"""
        feature_descriptions = {
            'gender': 'æ€§åˆ«',
            'date_of_birth': 'å‡ºç”Ÿå¹´æœˆæ—¥',
            'marital_status': 'å©šå§»çŠ¶å†µ',
            'occupation_type': 'èŒä¸šç±»å‹',
            'industry': 'æ‰€å±è¡Œä¸š',
            'family_structure': 'å®¶åº­ç»“æ„',
            'monthly_household_expense': 'æœˆåº¦å®¶åº­æ”¯å‡º',
            'is_family_financial_support': 'æ˜¯å¦ä¸ºå®¶åº­ç»æµæ”¯æŸ±',
            'annual_total_income': 'å¹´æ€»æ”¶å…¥',
            'income_stability': 'æ”¶å…¥ç¨³å®šæ€§',
            'annual_insurance_budget': 'å¹´åº¦ä¿é™©é¢„ç®—',
            'overall_health_status': 'æ•´ä½“å¥åº·çŠ¶å†µ'
        }

        features_to_ask = []
        for _, feature_name in priority_features:
            description = feature_descriptions.get(feature_name, feature_name)
            features_to_ask.append(description)

        # è·å–æŒä¹…åŒ–çš„å†å²é—®é¢˜ï¼Œç”¨äºé¿å…é‡å¤çš„é—®æ³•
        user_id = state["user_id"]
        asked_questions = self._get_user_questions_history(user_id)
        question_count = len(asked_questions)

        # æ„å»ºå†å²é—®é¢˜ä¸Šä¸‹æ–‡
        history_context = ""
        if asked_questions:
            recent_questions = asked_questions[-3:]  # åªæ˜¾ç¤ºæœ€è¿‘3ä¸ªé—®é¢˜
            history_context = f"""
ä¹‹å‰å·²è¯¢é—®è¿‡çš„é—®é¢˜ï¼š
{chr(10).join(f"- {q}" for q in recent_questions)}

è¯·é¿å…ä½¿ç”¨ç›¸åŒçš„å¼€å¤´ã€é—®æ³•æˆ–è¯­æ°”ã€‚è¦æ›´åŠ ç›´æ¥ã€è‡ªç„¶ï¼Œä¸è¦æ¯æ¬¡éƒ½æ‰“æ‹›å‘¼ã€‚
"""

        # æ ¹æ®é—®é¢˜æ¬¡æ•°è°ƒæ•´è¯­æ°”
        if question_count == 0:
            tone_instruction = "è¿™æ˜¯ç¬¬ä¸€æ¬¡è¯¢é—®ï¼Œå¯ä»¥ç¨å¾®æ­£å¼ä¸€äº›ï¼Œä½†è¦å‹å¥½ã€‚"
        elif question_count <= 2:
            tone_instruction = "ç»§ç»­å¯¹è¯ï¼Œè¯­æ°”è¦è‡ªç„¶ï¼Œå°±åƒæœ‹å‹é—´çš„äº¤æµã€‚"
        else:
            tone_instruction = "å·²ç»é—®è¿‡å‡ ä¸ªé—®é¢˜äº†ï¼Œè¦æ›´åŠ ç›´æ¥ç®€æ´ï¼Œé¿å…è¿‡å¤šçš„å®¢å¥—è¯ã€‚"

        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿é™©ç»çºªäººï¼Œä½ éœ€è¦é€šè¿‡ä¸€äº›è‡ªç„¶çš„å¯¹è¯ï¼Œä»å®¢æˆ·é‚£é‡Œäº†è§£ä¸€äº›éšç§ä¿¡æ¯ã€‚

è¦æ±‚ï¼š
1. æ¯æ¬¡åªé—®1ä¸ªé—®é¢˜ï¼Œé€‰æ‹©æœ€é‡è¦çš„ä¿¡æ¯
2. é—®é¢˜è¦ç›´æ¥ã€è‡ªç„¶ï¼Œé¿å…é‡å¤çš„å¼€å¤´å’Œé—®æ³•
3. ä¸è¦æ¯æ¬¡éƒ½è¯´"æ‚¨å¥½"ã€"ä¸ºäº†æ›´å¥½åœ°..."ç­‰å®¢å¥—è¯
4. å¯ä»¥å‡å®šç”¨æˆ·å¯¹ä¿é™©æœ‰å…´è¶£ï¼Œä¸éœ€è¦è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦è¿™äº›ä¿¡æ¯
5. å…è®¸ç”¨æˆ·é€‰æ‹©ä¸å›ç­”ï¼Œä½†ä¸è¦ä¸»åŠ¨æåŠ
6. è¯­æ°”è¦åƒçœŸå®çš„ä¿é™©ç»çºªäººï¼Œä¸“ä¸šä½†ä¸ç”Ÿç¡¬
7. {tone_instruction}

{'æ³¨æ„ï¼šå¦‚æœç”¨æˆ·æ˜¯å¥³æ€§ï¼Œå¯ä»¥é€‚å½“è¯¢é—®ç”Ÿè‚²ç›¸å…³çš„ä¿¡æ¯ã€‚' if ask_female_specific else ''}

{history_context}

ç°åœ¨éœ€è¦äº†è§£çš„ä¿¡æ¯ï¼š
{', '.join(features_to_ask)}

è¯·ç”Ÿæˆä¸€ä¸ªè‡ªç„¶ã€ç›´æ¥çš„é—®é¢˜ï¼š
"""
        return prompt

    async def analyze_profile(self, request: ProfileAnalysisRequest) -> AsyncGenerator[AgentResponse, None]:
        """æ‰§è¡Œç”¨æˆ·ç”»åƒåˆ†æ"""
        try:
            # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
            await db_manager.initialize()

            # åˆå§‹åŒ–çŠ¶æ€
            initial_state: ProfileAnalyzerState = {
                "user_id": request["user_id"],
                "session_id": request["session_id"],
                "history_chats": request["history_chats"],
                "custom_profile": request.get("custom_profile"),
                "existing_features": {},
                "extracted_features": {},
                "completion_status": {},
                "suggested_questions": None,
                "final_answer": None,
                "is_complete": False,
                "asked_questions_history": [],
                "asked_features_history": []
            }

            # æ‰§è¡Œå·¥ä½œæµ
            async for event in self.workflow.astream(initial_state):
                for node_name, node_output in event.items():
                    if node_name == "load_existing_features":
                        yield ThinkingResponse(
                            type="thinking",
                            content="æ­£åœ¨åŠ è½½æ‚¨çš„å·²æœ‰ä¿¡æ¯...",
                            step="load_features"
                        )
                    elif node_name == "analyze_conversation":
                        yield ThinkingResponse(
                            type="thinking",
                            content="æ­£åœ¨åˆ†ææ‚¨çš„å¯¹è¯å†…å®¹...",
                            step="analyze_conversation"
                        )
                    elif node_name == "extract_new_features":
                        yield ThinkingResponse(
                            type="thinking",
                            content="æ­£åœ¨æå–æ–°çš„ç”¨æˆ·ç‰¹å¾...",
                            step="extract_features"
                        )

                        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æå–çš„ç‰¹å¾ï¼Œå¦‚æœæœ‰åˆ™åŠæ—¶è¿”å›
                        extracted_features = node_output.get(
                            "extracted_features", {})
                        if extracted_features:
                            # å°†ç‰¹å¾è½¬æ¢ä¸º ProfileResponse æ ¼å¼
                            features_list = []
                            for feature_name, feature_data in extracted_features.items():
                                features_list.append({
                                    "category_name": feature_data["category_name"],
                                    "feature_name": feature_name,
                                    "feature_value": feature_data["feature_value"],
                                    "confidence": feature_data["confidence"],
                                    "skipped": feature_data.get("skipped", False)})

                            yield ProfileResponse(
                                type="profile",
                                features=features_list,
                                message=f"æˆåŠŸæå–äº† {len(features_list)} ä¸ªç”¨æˆ·ç‰¹å¾"
                            )
                    elif node_name == "update_database":
                        yield ThinkingResponse(
                            type="thinking",
                            content="æ­£åœ¨æ›´æ–°æ‚¨çš„ç”¨æˆ·ç”»åƒ...",
                            step="update_database"
                        )
                    elif node_name == "evaluate_completeness":
                        yield ThinkingResponse(
                            type="thinking",
                            content="æ­£åœ¨è¯„ä¼°ç”»åƒå®Œæ•´æ€§...",
                            step="evaluate_completeness"
                        )
                    elif node_name == "generate_questions":
                        yield AnswerResponse(
                            type="answer",
                            content=node_output.get("final_answer", "è¯·æä¾›æ›´å¤šä¿¡æ¯"),
                            data={
                                "questions": node_output.get("suggested_questions"),
                                "completion_status": node_output.get("completion_status", {})
                            }
                        )
                    elif node_name == "finalize_complete":
                        completion_status = node_output.get(
                            "completion_status", {})
                        profile_summary = completion_status.get(
                            "profile_summary", {})

                        yield UserProfileCompleteResponse(
                            type="profile_complete",
                            message=node_output.get(
                                "final_answer", "ç”¨æˆ·ç”»åƒåˆ†æå®Œæˆ"),
                            profile_summary=profile_summary.get("profile", {}),
                            completion_rate=profile_summary.get(
                                "completion_rate", 0.0)
                        )

        except Exception as e:
            logger.error(f"ç”¨æˆ·ç”»åƒåˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            yield ErrorResponse(
                type="error",
                error="åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
                details=str(e)
            )
