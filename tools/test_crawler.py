#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«æµ‹è¯•è„šæœ¬
ç”¨äºå¿«é€Ÿæµ‹è¯•çˆ¬è™«åŠŸèƒ½æ˜¯å¦æ­£å¸¸
"""

import sys
import os
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from shenlanbao_crawler import ShenlanbaoCrawler


def test_single_page():
    """æµ‹è¯•å•é¡µçˆ¬å–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å•é¡µçˆ¬å–åŠŸèƒ½...")
    
    crawler = ShenlanbaoCrawler()
    target_url = "https://www.shenlanbao.com/zhinan/list-23"
    
    # åªçˆ¬å–ç¬¬ä¸€é¡µ
    articles = crawler.crawl_all_pages(
        start_url=target_url,
        max_pages=1,
        include_content=False
    )
    
    if articles:
        print(f"âœ… å•é¡µæµ‹è¯•æˆåŠŸï¼Œè·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        # æ˜¾ç¤ºç¬¬ä¸€ç¯‡æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯
        first_article = articles[0]
        print(f"\nğŸ“° ç¬¬ä¸€ç¯‡æ–‡ç« ç¤ºä¾‹:")
        print(f"   æ ‡é¢˜: {first_article.get('title', 'N/A')}")
        print(f"   é“¾æ¥: {first_article.get('url', 'N/A')}")
        print(f"   æ—¶é—´: {first_article.get('publish_time', 'N/A')}")
        print(f"   é˜…è¯»: {first_article.get('views', 'N/A')}")
        print(f"   æè¿°: {first_article.get('description', 'N/A')[:150]}...")
        
        return True
    else:
        print("âŒ å•é¡µæµ‹è¯•å¤±è´¥ï¼Œæœªè·å–åˆ°æ–‡ç« ")
        return False


def test_pagination():
    """æµ‹è¯•åˆ†é¡µåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•åˆ†é¡µåŠŸèƒ½...")
    
    crawler = ShenlanbaoCrawler()
    target_url = "https://www.shenlanbao.com/zhinan/list-23"
    
    # çˆ¬å–å‰2é¡µ
    articles = crawler.crawl_all_pages(
        start_url=target_url,
        max_pages=2,
        include_content=False
    )
    
    if len(articles) > 10:  # å‡è®¾æ¯é¡µè‡³å°‘æœ‰10ç¯‡æ–‡ç« 
        print(f"âœ… åˆ†é¡µæµ‹è¯•æˆåŠŸï¼Œè·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        return True
    else:
        print(f"âš ï¸  åˆ†é¡µæµ‹è¯•å¯èƒ½æœ‰é—®é¢˜ï¼Œåªè·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        return False


def test_url_construction():
    """æµ‹è¯•URLæ„é€ åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•URLæ„é€ åŠŸèƒ½...")
    
    crawler = ShenlanbaoCrawler()
    
    # æµ‹è¯•ä¸åŒçš„URLæ¨¡å¼
    test_cases = [
        ("https://www.shenlanbao.com/zhinan/list-23", 2, "path"),
        ("https://www.shenlanbao.com/zhinan/list-23?param=1", 2, "query"),
        ("https://www.shenlanbao.com/zhinan/list-23", 3, "path")
    ]
    
    for base_url, page_num, pattern_type in test_cases:
        next_url = crawler.construct_next_page_url(base_url, page_num, pattern_type)
        print(f"  {base_url} -> {next_url}")
    
    print("âœ… URLæ„é€ æµ‹è¯•å®Œæˆ")
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ æ·±è“ä¿çˆ¬è™«åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    success_count = 0
    total_tests = 3
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("å•é¡µçˆ¬å–", test_single_page),
        ("URLæ„é€ ", test_url_construction),
        ("åˆ†é¡µåŠŸèƒ½", test_pagination)
    ]
    
    for test_name, test_func in tests:
        try:
            if test_func():
                success_count += 1
        except Exception as e:
            print(f"âŒ {test_name}æµ‹è¯•å‡ºé”™: {e}")
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print(f"\n{'='*50}")
    print(f"ğŸ¯ æµ‹è¯•å®Œæˆ: {success_count}/{total_tests} é€šè¿‡")
    
    if success_count == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬è™«åŠŸèƒ½æ­£å¸¸")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç½‘ç«™ç»“æ„å˜åŒ–")


if __name__ == "__main__":
    main() 