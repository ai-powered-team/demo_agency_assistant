#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±è“ä¿æ–‡ç« å†…å®¹æå–å™¨
ä»å·²çˆ¬å–çš„æ–‡ç« URLä¸­æå–æ­£æ–‡å†…å®¹ï¼Œä¿å­˜ä¸ºçº¯æ–‡æœ¬æ–‡ä»¶
"""

import json
import os
import time
import re
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from datetime import datetime


class ArticleContentExtractor:
    """æ–‡ç« å†…å®¹æå–å™¨"""
    
    def __init__(self, output_dir: str = "../data/text"):
        self.output_dir = Path(output_dir)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.shenlanbao.com/',
            'DNT': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin'
        })
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_articles = 0
        self.success_count = 0
        self.failed_count = 0
        self.failed_urls = []
        
    def get_page_content(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(retries):
            try:
                print(f"    ğŸ”— è®¿é—®: {url} (å°è¯• {attempt + 1}/{retries})")
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                return soup
                
            except requests.exceptions.RequestException as e:
                print(f"    âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    self.failed_urls.append({
                        'url': url,
                        'error': str(e),
                        'timestamp': datetime.now().isoformat()
                    })
        
        return None
    
    def extract_article_content(self, soup: BeautifulSoup, url: str) -> Optional[Dict[str, str]]:
        """ä»é¡µé¢ä¸­æå–æ–‡ç« å†…å®¹"""
        try:
            # æå–æ ‡é¢˜
            title = ""
            title_selectors = [
                'h1.title',
                'title',
                '.article-title',
                '.zhinan-title'
            ]
            
            for selector in title_selectors:
                title_element = soup.select_one(selector)
                if title_element:
                    title = title_element.get_text(strip=True)
                    break
            
            # å¦‚æœä»h1æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»titleæ ‡ç­¾ä¸­æå–
            if not title:
                title_tag = soup.find('title')
                if title_tag:
                    title = title_tag.get_text(strip=True)
            
            # æå–æ­£æ–‡å†…å®¹ - æ ¹æ®ç”¨æˆ·æä¾›çš„HTMLç»“æ„
            content = ""
            content_selectors = [
                '#zhinanLinkContains .rich-text-parse',  # ä¸»è¦çš„æ­£æ–‡å®¹å™¨
                '.rich-text-parse.underline-a',
                '.contains .rich-text-parse',
                '.article-content',
                '.zhinan-content',
                '#zhinanLinkContains',  # å¤‡ç”¨é€‰æ‹©å™¨
                '.contains'
            ]
            
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                    for unwanted in content_element.select('script, style, .advertisement, .ad, .banner'):
                        unwanted.decompose()
                    
                    # è·å–çº¯æ–‡æœ¬å†…å®¹
                    content = content_element.get_text(separator='\n', strip=True)
                    break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•æ›´å®½æ³›çš„é€‰æ‹©å™¨
            if not content:
                fallback_selectors = [
                    '.centerLeft',
                    '.zhinanContains',
                    'main',
                    'article',
                    '.main-content'
                ]
                
                for selector in fallback_selectors:
                    content_element = soup.select_one(selector)
                    if content_element:
                        # ç§»é™¤å¯¼èˆªã€å¹¿å‘Šç­‰ä¸éœ€è¦çš„å†…å®¹
                        for unwanted in content_element.select(
                            'nav, header, footer, .navigation, .breadcrumb, '
                            'script, style, .advertisement, .ad, .banner, '
                            '.sidebar, .related-articles, .comments'
                        ):
                            unwanted.decompose()
                        
                        content = content_element.get_text(separator='\n', strip=True)
                        if len(content) > 500:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                            break
            
            # æ¸…ç†å†…å®¹
            if content:
                # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
                content = re.sub(r'\n\s*\n', '\n\n', content)
                # ç§»é™¤è¡Œé¦–å°¾ç©ºç™½
                content = '\n'.join(line.strip() for line in content.split('\n'))
                # ç§»é™¤è¿‡çŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯å¯¼èˆªæˆ–æ— å…³å†…å®¹ï¼‰
                lines = content.split('\n')
                filtered_lines = []
                for line in lines:
                    if len(line) > 10 or not line or line.isspace():
                        filtered_lines.append(line)
                content = '\n'.join(filtered_lines)
            
            return {
                'title': title,
                'content': content,
                'url': url
            }
            
        except Exception as e:
            print(f"    âŒ å†…å®¹æå–å¤±è´¥: {e}")
            return None
    
    def save_article_text(self, article_data: Dict[str, str], filename: str) -> bool:
        """ä¿å­˜æ–‡ç« æ–‡æœ¬åˆ°æ–‡ä»¶"""
        try:
            file_path = self.output_dir / filename
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(f"æ ‡é¢˜: {article_data['title']}\n")
                f.write(f"URL: {article_data['url']}\n")
                f.write(f"æå–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 80 + "\n\n")
                f.write(article_data['content'])
            
            print(f"    âœ… ä¿å­˜æˆåŠŸ: {file_path}")
            return True
            
        except Exception as e:
            print(f"    âŒ ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def generate_filename(self, title: str, article_id: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤ä¸å®‰å…¨çš„å­—ç¬¦
        safe_title = re.sub(r'[<>:"/\\|?*]', '', title)
        safe_title = safe_title.replace('ï¼', '!').replace('ï¼Ÿ', '?')
        
        # é™åˆ¶é•¿åº¦
        if len(safe_title) > 50:
            safe_title = safe_title[:50]
        
        # ç”Ÿæˆæ–‡ä»¶å
        filename = f"{article_id}_{safe_title}.txt"
        return filename
    
    def extract_article_id_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–æ–‡ç« ID"""
        try:
            # URLæ ¼å¼: https://www.shenlanbao.com/zhinan/1784464437948104704
            parts = url.strip('/').split('/')
            article_id = parts[-1]
            return article_id
        except:
            # å¦‚æœæå–å¤±è´¥ï¼Œä½¿ç”¨URLçš„hashä½œä¸ºID
            return str(hash(url))[-10:]
    
    def process_articles_from_json(self, json_file: str, max_articles: Optional[int] = None) -> Dict:
        """ä»JSONæ–‡ä»¶å¤„ç†æ–‡ç« """
        print(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡ç« å†…å®¹æå–...")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“„ æ•°æ®æº: {json_file}")
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            articles = data.get('articles', [])
            self.total_articles = len(articles)
            
            if max_articles:
                articles = articles[:max_articles]
                print(f"ğŸ”¢ é™åˆ¶å¤„ç†æ•°é‡: {max_articles}")
            
            print(f"ğŸ“Š æ€»æ–‡ç« æ•°: {len(articles)}")
            print("=" * 60)
            
            for i, article in enumerate(articles, 1):
                url = article.get('url')
                title = article.get('title', 'æœªçŸ¥æ ‡é¢˜')
                
                if not url:
                    continue
                
                print(f"\nğŸ“° å¤„ç†ç¬¬ {i}/{len(articles)} ç¯‡æ–‡ç« :")
                print(f"    æ ‡é¢˜: {title}")
                
                # è·å–é¡µé¢å†…å®¹
                soup = self.get_page_content(url)
                if not soup:
                    print(f"    âŒ è·³è¿‡: æ— æ³•è·å–é¡µé¢å†…å®¹")
                    self.failed_count += 1
                    continue
                
                # æå–æ–‡ç« å†…å®¹
                article_data = self.extract_article_content(soup, url)
                if not article_data or not article_data['content']:
                    print(f"    âŒ è·³è¿‡: æ— æ³•æå–æ–‡ç« å†…å®¹")
                    self.failed_count += 1
                    continue
                
                # ç”Ÿæˆæ–‡ä»¶å
                article_id = self.extract_article_id_from_url(url)
                filename = self.generate_filename(article_data['title'], article_id)
                
                # ä¿å­˜æ–‡ä»¶
                if self.save_article_text(article_data, filename):
                    self.success_count += 1
                else:
                    self.failed_count += 1
                
                # è¯·æ±‚é—´éš”
                time.sleep(1)
            
            # ä¿å­˜å¤„ç†æŠ¥å‘Š
            self.save_processing_report()
            
        except Exception as e:
            print(f"âŒ å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
        
        return {
            'success': True,
            'total': self.total_articles,
            'success_count': self.success_count,
            'failed_count': self.failed_count
        }
    
    def save_processing_report(self):
        """ä¿å­˜å¤„ç†æŠ¥å‘Š"""
        report = {
            'processing_time': datetime.now().isoformat(),
            'total_articles': self.total_articles,
            'success_count': self.success_count,
            'failed_count': self.failed_count,
            'success_rate': f"{(self.success_count/self.total_articles*100):.1f}%" if self.total_articles > 0 else "0%",
            'failed_urls': self.failed_urls
        }
        
        report_file = self.output_dir / f"extraction_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š å¤„ç†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æå–æ·±è“ä¿æ–‡ç« æ­£æ–‡å†…å®¹')
    parser.add_argument('--json-file', '-f', 
                       default='../data/shenlanbao_é¿å‘æŒ‡å—_20250730_114302.json',
                       help='åŒ…å«æ–‡ç« URLçš„JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', '-o',
                       default='../data/text',
                       help='è¾“å‡ºæ–‡æœ¬æ–‡ä»¶çš„ç›®å½•')
    parser.add_argument('--max-articles', '-m',
                       type=int,
                       help='é™åˆ¶å¤„ç†çš„æ–‡ç« æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--test', '-t',
                       action='store_true',
                       help='æµ‹è¯•æ¨¡å¼ï¼Œåªå¤„ç†å‰5ç¯‡æ–‡ç« ')
    
    args = parser.parse_args()
    
    if args.test:
        args.max_articles = 5
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰5ç¯‡æ–‡ç« ")
    
    # æ£€æŸ¥JSONæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.json_file):
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {args.json_file}")
        print("è¯·å…ˆè¿è¡Œçˆ¬è™«è·å–æ–‡ç« åˆ—è¡¨")
        return
    
    # åˆ›å»ºæå–å™¨å¹¶å¼€å§‹å¤„ç†
    extractor = ArticleContentExtractor(output_dir=args.output_dir)
    result = extractor.process_articles_from_json(args.json_file, args.max_articles)
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    if result['success']:
        print(f"\n{'='*60}")
        print(f"ğŸ‰ å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»æ–‡ç« æ•°: {result['total']}")
        print(f"   æˆåŠŸæå–: {result['success_count']}")
        print(f"   å¤±è´¥æ•°é‡: {result['failed_count']}")
        print(f"   æˆåŠŸç‡: {(result['success_count']/result['total']*100):.1f}%")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {extractor.output_dir}")
        print(f"{'='*60}")
    else:
        print(f"âŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")


if __name__ == "__main__":
    main() 