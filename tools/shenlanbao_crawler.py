#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±è“ä¿ç½‘ç«™çˆ¬è™«
ç”¨äºçˆ¬å– https://www.shenlanbao.com/zhinan/list-23 é¡µé¢çš„æ–‡ç« å†…å®¹
æ”¯æŒåˆ†é¡µéå†å’Œå†…å®¹æå–
"""

import requests
import json
import time
import re
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
from bs4 import BeautifulSoup
from datetime import datetime


class ShenlanbaoCrawler:
    """æ·±è“ä¿ç½‘ç«™çˆ¬è™«ç±»"""
    
    def __init__(self):
        self.base_url = "https://www.shenlanbao.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://www.shenlanbao.com/',
            'DNT': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin'
        })
        self.articles = []
        
    def get_page_content(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
        for attempt in range(retries):
            try:
                print(f"  æ­£åœ¨è¯·æ±‚: {url} (å°è¯• {attempt + 1}/{retries})")
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # éªŒè¯é¡µé¢æ˜¯å¦æœ‰æ•ˆï¼ˆåŒ…å«æ–‡ç« å†…å®¹æˆ–åˆ†é¡µå™¨ï¼‰
                if soup.find_all('div', class_='article-item') or soup.find('ul', {'id': 'm_fenye'}):
                    return soup
                else:
                    print(f"  é¡µé¢æ— æ•ˆå†…å®¹ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    return None
                    
            except requests.exceptions.RequestException as e:
                print(f"  è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    print(f"  è·å–é¡µé¢å†…å®¹å¤±è´¥: {url}")
        
        return None
    
    def extract_article_info(self, article_element) -> Optional[Dict]:
        """ä»æ–‡ç« å…ƒç´ ä¸­æå–æ–‡ç« ä¿¡æ¯"""
        try:
            article_info = {}
            
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_link = article_element.find('a', class_='title') or article_element.find('a', class_='style-oneline')
            if title_link:
                article_info['title'] = title_link.get_text(strip=True)
                article_info['url'] = urljoin(self.base_url, title_link.get('href', ''))
            else:
                return None
            
            # æå–æè¿°
            desc_element = article_element.find('div', class_='desc') or article_element.find('div', class_='style-mutiline')
            if desc_element:
                article_info['description'] = desc_element.get_text(strip=True)
            
            # æå–å‘å¸ƒæ—¶é—´
            time_element = article_element.find('div', class_='publish-time')
            if time_element:
                time_span = time_element.find('span')
                if time_span:
                    article_info['publish_time'] = time_span.get_text(strip=True)
            
            # æå–é˜…è¯»é‡
            view_element = article_element.find('div', class_='view')
            if view_element:
                view_span = view_element.find('span')
                if view_span:
                    article_info['views'] = view_span.get_text(strip=True)
            
            # æå–å›¾ç‰‡
            img_element = article_element.find('img')
            if img_element:
                article_info['image_url'] = img_element.get('src', '')
            
            return article_info
            
        except Exception as e:
            print(f"  æå–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def get_article_content(self, article_url: str) -> Optional[str]:
        """è·å–æ–‡ç« è¯¦ç»†å†…å®¹"""
        try:
            soup = self.get_page_content(article_url)
            if not soup:
                return None
            
            # å°è¯•å¤šç§å¯èƒ½çš„å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                '.article-content',
                '.content',
                '.main-content',
                '.post-content',
                'article',
                '.article-body',
                '.zhishi-content',
                '.detail-content'
            ]
            
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    # æ¸…ç†å†…å®¹ï¼Œç§»é™¤è„šæœ¬å’Œæ ·å¼
                    for script in content_element(["script", "style"]):
                        script.decompose()
                    return content_element.get_text(strip=True)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸“é—¨çš„å†…å®¹åŒºåŸŸï¼Œå°è¯•æå–ä¸»è¦æ–‡æœ¬
            body = soup.find('body')
            if body:
                for script in body(["script", "style", "nav", "header", "footer"]):
                    script.decompose()
                return body.get_text(strip=True)[:2000]  # é™åˆ¶é•¿åº¦
                
        except Exception as e:
            print(f"  è·å–æ–‡ç« å†…å®¹å¤±è´¥: {article_url}, é”™è¯¯: {e}")
        
        return None
    
    def parse_page(self, url: str) -> List[Dict]:
        """è§£æå•ä¸ªé¡µé¢çš„æ–‡ç« åˆ—è¡¨"""
        soup = self.get_page_content(url)
        if not soup:
            return []
        
        articles = []
        
        # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨å®¹å™¨
        article_items = soup.find_all('div', class_='article-item')
        
        for item in article_items:
            article_info = self.extract_article_info(item)
            if article_info:
                articles.append(article_info)
                print(f"  âœ“ æå–æ–‡ç« : {article_info['title']}")
        
        return articles
    
    def detect_pagination_pattern(self, url: str) -> Tuple[str, int]:
        """æ£€æµ‹åˆ†é¡µURLæ¨¡å¼"""
        parsed = urlparse(url)
        
        # å¸¸è§çš„åˆ†é¡µURLæ¨¡å¼
        patterns = [
            # æ¨¡å¼1: /list-23/2, /list-23/3 (è·¯å¾„å½¢å¼)
            (r'/list-(\d+)$', 'path'),
            # æ¨¡å¼2: ?page=1, ?page=2 (æŸ¥è¯¢å‚æ•°)
            (r'\?.*page=', 'query'),
            # æ¨¡å¼3: #page1, #page2 (é”šç‚¹)
            (r'#page', 'hash')
        ]
        
        for pattern, ptype in patterns:
            if re.search(pattern, url):
                return ptype, 1
        
        return 'path', 1  # é»˜è®¤ä½¿ç”¨è·¯å¾„æ¨¡å¼
    
    def construct_next_page_url(self, base_url: str, page_num: int, pattern_type: str = 'path') -> str:
        """æ„é€ ä¸‹ä¸€é¡µURL"""
        try:
            if pattern_type == 'path':
                # è·¯å¾„æ¨¡å¼: /zhinan/list-23 -> /zhinan/list-23/2
                if base_url.endswith('/'):
                    return f"{base_url.rstrip('/')}/{page_num}"
                else:
                    return f"{base_url}/{page_num}"
            
            elif pattern_type == 'query':
                # æŸ¥è¯¢å‚æ•°æ¨¡å¼
                parsed = urlparse(base_url)
                query_params = parse_qs(parsed.query)
                query_params['page'] = [str(page_num)]
                new_query = urlencode(query_params, doseq=True)
                return urlunparse((
                    parsed.scheme, parsed.netloc, parsed.path,
                    parsed.params, new_query, parsed.fragment
                ))
            
            else:  # hashæ¨¡å¼
                parsed = urlparse(base_url)
                return urlunparse((
                    parsed.scheme, parsed.netloc, parsed.path,
                    parsed.params, parsed.query, f"page{page_num}"
                ))
                
        except Exception as e:
            print(f"  æ„é€ URLå¤±è´¥: {e}")
            return f"{base_url.rstrip('/')}/{page_num}"  # å¤‡ç”¨æ–¹æ¡ˆ
    
    def find_pagination_info(self, soup: BeautifulSoup, current_url: str) -> Dict:
        """æ™ºèƒ½åˆ†æåˆ†é¡µä¿¡æ¯"""
        pagination_info = {
            'current_page': 1,
            'total_pages': 1,
            'has_next': False,
            'next_url': None,
            'pattern_type': 'path'
        }
        
        try:
            # 1. å°è¯•ä»åˆ†é¡µå™¨DOMä¸­è·å–ä¿¡æ¯
            pagination = soup.find('ul', {'id': 'm_fenye'}) or soup.find('ul', class_='app-pagination')
            
            if pagination:
                # æŸ¥æ‰¾é¡µç é“¾æ¥
                page_links = pagination.find_all('a')
                page_numbers = []
                
                for link in page_links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    # æå–é¡µç 
                    if text.isdigit():
                        page_numbers.append(int(text))
                    elif 'ä¸‹ä¸€é¡µ' in text or 'next' in text.lower():
                        pagination_info['has_next'] = True
                        pagination_info['next_url'] = urljoin(self.base_url, href)
                    
                    # ä»hrefä¸­æå–é¡µç ä¿¡æ¯
                    if href:
                        page_match = re.search(r'/(\d+)$', href) or re.search(r'page=(\d+)', href)
                        if page_match:
                            page_numbers.append(int(page_match.group(1)))
                
                if page_numbers:
                    pagination_info['total_pages'] = max(page_numbers)
            
            # 2. ä»URLä¸­æå–å½“å‰é¡µç 
            current_page_match = re.search(r'/(\d+)$', current_url) or re.search(r'page=(\d+)', current_url)
            if current_page_match:
                pagination_info['current_page'] = int(current_page_match.group(1))
            
            # 3. æ£€æµ‹URLæ¨¡å¼
            pattern_type, _ = self.detect_pagination_pattern(current_url)
            pagination_info['pattern_type'] = pattern_type
            
            # 4. å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„åˆ†é¡µä¿¡æ¯ï¼Œä½¿ç”¨æ™ºèƒ½æ¨æµ‹
            if pagination_info['total_pages'] == 1:
                # é€šè¿‡é¡µé¢ç»“æ„æ¨æµ‹æ˜¯å¦æœ‰æ›´å¤šé¡µé¢
                article_count = len(soup.find_all('div', class_='article-item'))
                if article_count >= 10:  # å‡è®¾æ¯é¡µè‡³å°‘10ç¯‡æ–‡ç« 
                    pagination_info['total_pages'] = 50  # è®¾ç½®ä¸€ä¸ªåˆç†çš„æœ€å¤§å€¼
                    pagination_info['has_next'] = True
            
        except Exception as e:
            print(f"  åˆ†æåˆ†é¡µä¿¡æ¯å¤±è´¥: {e}")
        
        return pagination_info
    
    def crawl_all_pages(self, start_url: str, max_pages: int = None, include_content: bool = False) -> List[Dict]:
        """çˆ¬å–æ‰€æœ‰åˆ†é¡µçš„æ–‡ç« """
        all_articles = []
        current_url = start_url
        page_count = 1
        consecutive_empty_pages = 0
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å–: {start_url}")
        print(f"ğŸ“„ æœ€å¤§é¡µæ•°: {max_pages if max_pages else 'æ— é™åˆ¶'}")
        print(f"ğŸ“ åŒ…å«å†…å®¹: {'æ˜¯' if include_content else 'å¦'}")
        print("=" * 60)
        
        # æ£€æµ‹åˆ†é¡µæ¨¡å¼
        pattern_type, _ = self.detect_pagination_pattern(start_url)
        base_url = start_url
        
        while current_url and (max_pages is None or page_count <= max_pages):
            print(f"\nğŸ“– æ­£åœ¨çˆ¬å–ç¬¬ {page_count} é¡µ...")
            print(f"ğŸ”— URL: {current_url}")
            
            soup = self.get_page_content(current_url)
            if not soup:
                print(f"  âŒ é¡µé¢åŠ è½½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€é¡µ...")
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= 3:
                    print(f"  ğŸ›‘ è¿ç»­3é¡µå¤±è´¥ï¼Œåœæ­¢çˆ¬å–")
                    break
            else:
                consecutive_empty_pages = 0
            
            # è§£æå½“å‰é¡µé¢çš„æ–‡ç« 
            if soup:
                page_articles = self.parse_page(current_url)
                
                if not page_articles:
                    print(f"  âš ï¸  ç¬¬ {page_count} é¡µæ— æ–‡ç« å†…å®¹")
                    consecutive_empty_pages += 1
                    if consecutive_empty_pages >= 2:
                        print(f"  ğŸ›‘ è¿ç»­ç©ºé¡µé¢ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                        break
                else:
                    consecutive_empty_pages = 0
                    
                    # å¦‚æœéœ€è¦è·å–æ–‡ç« è¯¦ç»†å†…å®¹
                    if include_content:
                        for i, article in enumerate(page_articles):
                            if article.get('url'):
                                print(f"  ğŸ“° è·å–æ–‡ç« å†…å®¹ ({i+1}/{len(page_articles)}): {article['title'][:50]}...")
                                article['content'] = self.get_article_content(article['url'])
                                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                    
                    all_articles.extend(page_articles)
                    print(f"  âœ… ç¬¬ {page_count} é¡µè·å–åˆ° {len(page_articles)} ç¯‡æ–‡ç« ")
                
                # è·å–åˆ†é¡µä¿¡æ¯
                pagination_info = self.find_pagination_info(soup, current_url)
                print(f"  ğŸ“Š åˆ†é¡µä¿¡æ¯: å½“å‰ç¬¬{pagination_info['current_page']}é¡µï¼Œæ€»è®¡{pagination_info['total_pages']}é¡µ")
            
            # æ„é€ ä¸‹ä¸€é¡µURL
            next_page_num = page_count + 1
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­
            if soup and pagination_info.get('has_next') and pagination_info.get('next_url'):
                current_url = pagination_info['next_url']
            elif next_page_num <= pagination_info.get('total_pages', 1):
                current_url = self.construct_next_page_url(base_url, next_page_num, pattern_type)
            else:
                # å°è¯•æ„é€ å¯èƒ½çš„ä¸‹ä¸€é¡µURL
                potential_next_url = self.construct_next_page_url(base_url, next_page_num, pattern_type)
                
                # å¿«é€Ÿæ£€æŸ¥ä¸‹ä¸€é¡µæ˜¯å¦å­˜åœ¨ï¼ˆHEADè¯·æ±‚ï¼‰
                try:
                    test_response = self.session.head(potential_next_url, timeout=5)
                    if test_response.status_code == 200:
                        current_url = potential_next_url
                    else:
                        print(f"  ğŸ ä¸‹ä¸€é¡µä¸å­˜åœ¨ (HTTP {test_response.status_code})ï¼Œç»“æŸçˆ¬å–")
                        break
                except:
                    print(f"  ğŸ æ— æ³•è®¿é—®ä¸‹ä¸€é¡µï¼Œç»“æŸçˆ¬å–")
                    break
            
            page_count += 1
            time.sleep(1)  # è¯·æ±‚é—´éš”
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(all_articles)} ç¯‡æ–‡ç« ")
        print(f"ğŸ“„ å…±çˆ¬å– {page_count - 1} é¡µ")
        print(f"{'='*60}")
        
        return all_articles
    
    def save_to_json(self, articles: List[Dict], filename: str):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'crawl_time': datetime.now().isoformat(),
                    'total_articles': len(articles),
                    'articles': articles
                }, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    crawler = ShenlanbaoCrawler()
    
    # è¦çˆ¬å–çš„URL
    target_url = "https://www.shenlanbao.com/zhinan/list-23"
    
    # çˆ¬å–æ‰€æœ‰é¡µé¢çš„æ–‡ç« ï¼ˆä¸åŒ…å«è¯¦ç»†å†…å®¹ï¼‰
    print("ğŸ•·ï¸  æ·±è“ä¿æ–‡ç« çˆ¬è™«å¯åŠ¨...")
    articles = crawler.crawl_all_pages(
        start_url=target_url,
        max_pages=5,  # é™åˆ¶æœ€å¤§é¡µæ•°ï¼Œé¿å…çˆ¬å–æ—¶é—´è¿‡é•¿
        include_content=False  # è®¾ç½®ä¸ºTrueå¯ä»¥è·å–æ–‡ç« è¯¦ç»†å†…å®¹ï¼Œä½†ä¼šå¢åŠ çˆ¬å–æ—¶é—´
    )
    
    # ä¿å­˜ç»“æœ
    output_file = f"../data/shenlanbao_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    crawler.save_to_json(articles, output_file)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    if articles:
        print(f"\nğŸ“ˆ ç¤ºä¾‹æ–‡ç« :")
        for i, article in enumerate(articles[:3]):  # æ˜¾ç¤ºå‰3ç¯‡æ–‡ç« 
            print(f"\n{i+1}. ğŸ“° {article.get('title', 'N/A')}")
            print(f"   ğŸ”— {article.get('url', 'N/A')}")
            print(f"   ğŸ“… {article.get('publish_time', 'N/A')}")
            print(f"   ğŸ‘ï¸  {article.get('views', 'N/A')} æ¬¡é˜…è¯»")
            desc = article.get('description', 'N/A')
            if len(desc) > 100:
                desc = desc[:100] + "..."
            print(f"   ğŸ“ {desc}")


if __name__ == "__main__":
    main() 