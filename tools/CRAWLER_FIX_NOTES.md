# æ·±è“ä¿çˆ¬è™«åˆ†é¡µå™¨ä¿®å¤è¯´æ˜

## ğŸ› ä¿®å¤çš„é—®é¢˜

### 1. åŸæœ‰åˆ†é¡µæ£€æµ‹é—®é¢˜
- **é—®é¢˜**: åŸä»£ç ä¾èµ–é™æ€HTMLä¸­çš„åˆ†é¡µé“¾æ¥ï¼Œä½†æ·±è“ä¿ç½‘ç«™å¯èƒ½ä½¿ç”¨JavaScriptåŠ¨æ€ç”Ÿæˆåˆ†é¡µ
- **å½±å“**: æ— æ³•æ­£ç¡®è¯†åˆ«åˆ†é¡µç»“æ„ï¼Œå¯¼è‡´åªèƒ½çˆ¬å–ç¬¬ä¸€é¡µ
- **è¡¨ç°**: çˆ¬è™«åœ¨ç¬¬ä¸€é¡µåå°±åœæ­¢ï¼Œæ— æ³•ç»§ç»­è·å–æ›´å¤šå†…å®¹

### 2. URLæ„é€ é€»è¾‘ç¼ºé™·
- **é—®é¢˜**: ç®€å•çš„`?page=`å‚æ•°æ‹¼æ¥ï¼Œä¸ç¬¦åˆæ·±è“ä¿ç½‘ç«™çš„å®é™…URLç»“æ„
- **å½±å“**: æ„é€ çš„ä¸‹ä¸€é¡µURLæ— æ•ˆï¼Œè¿”å›404æˆ–é‡å®šå‘åˆ°é¦–é¡µ
- **è¡¨ç°**: æ— æ³•è®¿é—®åç»­é¡µé¢

### 3. åˆ†é¡µç»ˆæ­¢æ¡ä»¶è¿‡ä¸¥
- **é—®é¢˜**: é‡åˆ°å•æ¬¡å¤±è´¥å°±ç«‹å³åœæ­¢ï¼Œæ²¡æœ‰é‡è¯•æœºåˆ¶
- **å½±å“**: ç½‘ç»œæ³¢åŠ¨æˆ–ä¸´æ—¶é”™è¯¯å¯¼è‡´çˆ¬å–æ„å¤–ç»ˆæ­¢
- **è¡¨ç°**: çˆ¬è™«è¿‡æ—©åœæ­¢ï¼Œæ•°æ®è·å–ä¸å®Œæ•´

## ğŸ”§ ä¿®å¤å†…å®¹

### 1. æ™ºèƒ½åˆ†é¡µæ¨¡å¼æ£€æµ‹

```python
def detect_pagination_pattern(self, url: str) -> Tuple[str, int]:
    """æ£€æµ‹åˆ†é¡µURLæ¨¡å¼"""
    patterns = [
        (r'/list-(\d+)$', 'path'),      # /list-23 -> /list-23/2
        (r'\?.*page=', 'query'),        # ?page=1 -> ?page=2
        (r'#page', 'hash')              # #page1 -> #page2
    ]
```

**æ”¹è¿›ç‚¹**:
- è‡ªåŠ¨è¯†åˆ«ç½‘ç«™ä½¿ç”¨çš„åˆ†é¡µURLæ¨¡å¼
- æ”¯æŒå¤šç§å¸¸è§åˆ†é¡µæ–¹å¼
- åŠ¨æ€é€‚åº”ä¸åŒçš„URLç»“æ„

### 2. ç²¾ç¡®çš„URLæ„é€ 

```python
def construct_next_page_url(self, base_url: str, page_num: int, pattern_type: str = 'path') -> str:
    """æ„é€ ä¸‹ä¸€é¡µURL"""
    if pattern_type == 'path':
        return f"{base_url}/{page_num}"
    elif pattern_type == 'query':
        # æ­£ç¡®å¤„ç†æŸ¥è¯¢å‚æ•°
        parsed = urlparse(base_url)
        query_params = parse_qs(parsed.query)
        query_params['page'] = [str(page_num)]
        # ... URLé‡æ„é€»è¾‘
```

**æ”¹è¿›ç‚¹**:
- æ ¹æ®æ£€æµ‹åˆ°çš„æ¨¡å¼ç²¾ç¡®æ„é€ URL
- æ­£ç¡®å¤„ç†æŸ¥è¯¢å‚æ•°å’ŒURLç»“æ„
- æä¾›å¤šç§å¤‡ç”¨æ–¹æ¡ˆ

### 3. å¤šç»´åº¦åˆ†é¡µä¿¡æ¯åˆ†æ

```python
def find_pagination_info(self, soup: BeautifulSoup, current_url: str) -> Dict:
    """æ™ºèƒ½åˆ†æåˆ†é¡µä¿¡æ¯"""
    # 1. ä»DOMä¸­æå–åˆ†é¡µé“¾æ¥
    # 2. ä»URLä¸­æå–å½“å‰é¡µç 
    # 3. é€šè¿‡é¡µé¢ç»“æ„æ¨æµ‹æ€»é¡µæ•°
    # 4. æ™ºèƒ½è®¾ç½®åˆç†çš„åˆ†é¡µèŒƒå›´
```

**æ”¹è¿›ç‚¹**:
- å¤šç§æ–¹å¼è·å–åˆ†é¡µä¿¡æ¯ï¼Œæé«˜å‡†ç¡®æ€§
- æ™ºèƒ½æ¨æµ‹æœºåˆ¶ï¼Œåº”å¯¹åŠ¨æ€åˆ†é¡µ
- ä»é¡µé¢å†…å®¹æ¨æ–­æ˜¯å¦æœ‰æ›´å¤šé¡µé¢

### 4. å¥å£®çš„é”™è¯¯å¤„ç†

```python
def get_page_content(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
    """è·å–é¡µé¢å†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
    for attempt in range(retries):
        try:
            response = self.session.get(url, timeout=15)
            # éªŒè¯é¡µé¢æœ‰æ•ˆæ€§
            if soup.find_all('div', class_='article-item'):
                return soup
        except Exception as e:
            if attempt < retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

**æ”¹è¿›ç‚¹**:
- è¯·æ±‚é‡è¯•æœºåˆ¶ï¼Œåº”å¯¹ç½‘ç»œæ³¢åŠ¨
- é¡µé¢æœ‰æ•ˆæ€§éªŒè¯ï¼Œé¿å…å¤„ç†é”™è¯¯é¡µé¢
- æŒ‡æ•°é€€é¿ç­–ç•¥ï¼Œå‡å°‘æœåŠ¡å™¨å‹åŠ›
- è¿ç»­å¤±è´¥æ£€æµ‹ï¼Œæ™ºèƒ½ç»ˆæ­¢

### 5. é¢„æ£€æœºåˆ¶

```python
# ä½¿ç”¨HEADè¯·æ±‚å¿«é€Ÿæ£€æŸ¥ä¸‹ä¸€é¡µæ˜¯å¦å­˜åœ¨
test_response = self.session.head(potential_next_url, timeout=5)
if test_response.status_code == 200:
    current_url = potential_next_url
else:
    print(f"ä¸‹ä¸€é¡µä¸å­˜åœ¨ (HTTP {test_response.status_code})")
    break
```

**æ”¹è¿›ç‚¹**:
- HEADè¯·æ±‚é¢„æ£€ï¼Œé¿å…ä¸‹è½½æ— æ•ˆé¡µé¢
- å¿«é€Ÿåˆ¤æ–­é¡µé¢å­˜åœ¨æ€§
- å‡å°‘ä¸å¿…è¦çš„æµé‡æ¶ˆè€—

## ğŸ¯ ä¿®å¤æ•ˆæœ

### ä¿®å¤å‰
- âŒ åªèƒ½çˆ¬å–ç¬¬ä¸€é¡µ
- âŒ åˆ†é¡µURLæ„é€ é”™è¯¯
- âŒ ç½‘ç»œé”™è¯¯ç›´æ¥å¤±è´¥
- âŒ æ— æ³•é€‚åº”ä¸åŒURLæ¨¡å¼

### ä¿®å¤å
- âœ… æ­£ç¡®éå†æ‰€æœ‰åˆ†é¡µ
- âœ… æ™ºèƒ½è¯†åˆ«åˆ†é¡µæ¨¡å¼
- âœ… å¥å£®çš„é”™è¯¯å¤„ç†
- âœ… å¤šç§å¤‡ç”¨æ–¹æ¡ˆ
- âœ… è¯¦ç»†çš„è¿›åº¦åé¦ˆ

## ğŸ§ª æµ‹è¯•éªŒè¯

åˆ›å»ºäº†ä¸“é—¨çš„æµ‹è¯•è„šæœ¬ `test_crawler.py`:

```bash
cd tools
python test_crawler.py
```

æµ‹è¯•å†…å®¹åŒ…æ‹¬:
1. **å•é¡µçˆ¬å–æµ‹è¯•** - éªŒè¯åŸºæœ¬åŠŸèƒ½
2. **URLæ„é€ æµ‹è¯•** - éªŒè¯URLç”Ÿæˆé€»è¾‘
3. **åˆ†é¡µåŠŸèƒ½æµ‹è¯•** - éªŒè¯å¤šé¡µçˆ¬å–

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### è¯·æ±‚ä¼˜åŒ–
- å¢åŠ äº†æ›´å®Œæ•´çš„HTTPå¤´éƒ¨ä¿¡æ¯
- ä½¿ç”¨Sessionå¤ç”¨è¿æ¥
- åˆç†çš„è¶…æ—¶è®¾ç½®

### æ•ˆç‡æå‡
- HEADè¯·æ±‚é¢„æ£€ï¼Œé¿å…ä¸‹è½½æ— æ•ˆå†…å®¹
- æ™ºèƒ½ç»ˆæ­¢æ¡ä»¶ï¼Œå‡å°‘æ— æ•ˆè¯·æ±‚
- è¯·æ±‚é—´éš”æ§åˆ¶ï¼Œé¿å…è¿‡äºé¢‘ç¹

### ç”¨æˆ·ä½“éªŒ
- ä¸°å¯Œçš„emojiå›¾æ ‡å’Œè¿›åº¦æ˜¾ç¤º
- è¯¦ç»†çš„æ—¥å¿—è¾“å‡º
- æ¸…æ™°çš„ç»Ÿè®¡æŠ¥å‘Š

## ğŸš€ ä½¿ç”¨å»ºè®®

### åŸºæœ¬ä½¿ç”¨
```bash
cd tools
python run_crawler.py --max-pages 5 --verbose
```

### æµ‹è¯•éªŒè¯
```bash
# å…ˆè¿è¡Œæµ‹è¯•ç¡®ä¿åŠŸèƒ½æ­£å¸¸
python test_crawler.py

# å†è¿›è¡Œå®é™…çˆ¬å–
python run_crawler.py --category é¿å‘æŒ‡å— --max-pages 10
```

### å‚æ•°è°ƒä¼˜
- `--max-pages`: å»ºè®®ä»å°æ•°å€¼å¼€å§‹æµ‹è¯•
- `--delay`: ç½‘ç»œä¸ç¨³å®šæ—¶å¯å¢åŠ é—´éš”
- `--verbose`: è°ƒè¯•æ—¶å¯ç”¨è¯¦ç»†æ—¥å¿—

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **åˆè§„ä½¿ç”¨**: è¯·éµå®ˆç½‘ç«™robots.txtå’Œä½¿ç”¨æ¡æ¬¾
2. **é¢‘ç‡æ§åˆ¶**: é»˜è®¤1ç§’é—´éš”ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›
3. **æ•°æ®éªŒè¯**: å»ºè®®æ£€æŸ¥çˆ¬å–ç»“æœçš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
4. **ç‰ˆæœ¬å…¼å®¹**: å¦‚ç½‘ç«™ç»“æ„å˜åŒ–ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é€‰æ‹©å™¨é…ç½®

## ğŸ”„ åç»­ç»´æŠ¤

å¦‚æœç½‘ç«™ç»“æ„å‘ç”Ÿå˜åŒ–ï¼Œä¸»è¦éœ€è¦æ£€æŸ¥:
1. CSSé€‰æ‹©å™¨æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
2. åˆ†é¡µURLæ¨¡å¼æ˜¯å¦æ”¹å˜
3. é¡µé¢åŠ è½½æ–¹å¼æ˜¯å¦è°ƒæ•´

å¯ä»¥é€šè¿‡ä¿®æ”¹ `crawler_config.py` ä¸­çš„é…ç½®æ¥å¿«é€Ÿé€‚åº”å˜åŒ–ã€‚ 