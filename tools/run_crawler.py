#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±è“ä¿çˆ¬è™«å‘½ä»¤è¡Œè¿è¡Œè„šæœ¬
æ”¯æŒå¤šç§å‚æ•°é…ç½®ï¼Œæ–¹ä¾¿ä½¿ç”¨
"""

import argparse
import sys
import os
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from shenlanbao_crawler import ShenlanbaoCrawler
from crawler_config import CRAWLER_CONFIG, SHENLANBAO_CONFIG


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='æ·±è“ä¿ç½‘ç«™æ–‡ç« çˆ¬è™«å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨ - çˆ¬å–é¿å‘æŒ‡å—
  python run_crawler.py

  # æŒ‡å®šåˆ†ç±»
  python run_crawler.py --category å°ç™½å…¥é—¨

  # é™åˆ¶é¡µæ•°å’ŒåŒ…å«å†…å®¹
  python run_crawler.py --max-pages 5 --include-content

  # æŒ‡å®šè‡ªå®šä¹‰URL
  python run_crawler.py --url https://www.shenlanbao.com/zhinan/list-20

æ”¯æŒçš„åˆ†ç±»:
  - é¿å‘æŒ‡å— (é»˜è®¤)
  - å°ç™½å…¥é—¨
  - è´­é™©å¿…çœ‹
  - ç†èµ”ç§‘æ™®
        """
    )
    
    parser.add_argument(
        '--category', '-c',
        choices=['é¿å‘æŒ‡å—', 'å°ç™½å…¥é—¨', 'è´­é™©å¿…çœ‹', 'ç†èµ”ç§‘æ™®'],
        default='é¿å‘æŒ‡å—',
        help='è¦çˆ¬å–çš„æ–‡ç« åˆ†ç±» (é»˜è®¤: é¿å‘æŒ‡å—)'
    )
    
    parser.add_argument(
        '--url', '-u',
        type=str,
        help='è‡ªå®šä¹‰è¦çˆ¬å–çš„URL (å¦‚æœæŒ‡å®šæ­¤å‚æ•°ï¼Œ--categoryå°†è¢«å¿½ç•¥)'
    )
    
    parser.add_argument(
        '--max-pages', '-m',
        type=int,
        default=CRAWLER_CONFIG['max_pages'],
        help=f'æœ€å¤§çˆ¬å–é¡µæ•° (é»˜è®¤: {CRAWLER_CONFIG["max_pages"]})'
    )
    
    parser.add_argument(
        '--include-content', '-i',
        action='store_true',
        default=CRAWLER_CONFIG['include_content'],
        help='æ˜¯å¦è·å–æ–‡ç« è¯¦ç»†å†…å®¹ (é»˜è®¤: å¦)'
    )
    
    parser.add_argument(
        '--delay', '-d',
        type=float,
        default=CRAWLER_CONFIG['request_delay'],
        help=f'è¯·æ±‚é—´éš”æ—¶é—´(ç§’) (é»˜è®¤: {CRAWLER_CONFIG["request_delay"]})'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ)'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # ç¡®å®šç›®æ ‡URL
    if args.url:
        target_url = args.url
        category_name = "è‡ªå®šä¹‰"
    else:
        target_url = SHENLANBAO_CONFIG['target_urls'][args.category]
        category_name = args.category
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
    if args.output:
        output_file = args.output
    else:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"shenlanbao_{category_name}_{timestamp}.json"
        output_file = os.path.join(CRAWLER_CONFIG['output_dir'], filename)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("=" * 60)
    print("æ·±è“ä¿ç½‘ç«™çˆ¬è™«å·¥å…·")
    print("=" * 60)
    print(f"åˆ†ç±»: {category_name}")
    print(f"URL: {target_url}")
    print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    print(f"åŒ…å«å†…å®¹: {'æ˜¯' if args.include_content else 'å¦'}")
    print(f"è¯·æ±‚é—´éš”: {args.delay}ç§’")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ShenlanbaoCrawler()
    
    # æ›´æ–°é…ç½®
    crawler.session.headers.update(CRAWLER_CONFIG['headers'])
    
    try:
        # å¼€å§‹çˆ¬å–
        articles = crawler.crawl_all_pages(
            start_url=target_url,
            max_pages=args.max_pages,
            include_content=args.include_content
        )
        
        if not articles:
            print("\nâŒ æœªè·å–åˆ°ä»»ä½•æ–‡ç« ï¼Œè¯·æ£€æŸ¥URLæˆ–ç½‘ç»œè¿æ¥")
            return
        
        # ä¿å­˜ç»“æœ
        crawler.save_to_json(articles, output_file)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        print("çˆ¬å–å®Œæˆç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"âœ… æ€»æ–‡ç« æ•°: {len(articles)}")
        print(f"âœ… è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        # æ˜¾ç¤ºç¤ºä¾‹æ–‡ç« ï¼ˆå¦‚æœå¯ç”¨è¯¦ç»†æ¨¡å¼ï¼‰
        if args.verbose and articles:
            print(f"\nç¤ºä¾‹æ–‡ç« :")
            for i, article in enumerate(articles[:3]):
                print(f"\n{i+1}. {article.get('title', 'N/A')}")
                print(f"   ğŸ“… {article.get('publish_time', 'N/A')}")
                print(f"   ğŸ‘ï¸  {article.get('views', 'N/A')} æ¬¡é˜…è¯»")
                print(f"   ğŸ”— {article.get('url', 'N/A')}")
                desc = article.get('description', 'N/A')
                if len(desc) > 100:
                    desc = desc[:100] + "..."
                print(f"   ğŸ“ {desc}")
        
        print(f"\nğŸ‰ çˆ¬å–æˆåŠŸå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main() 